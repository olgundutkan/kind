Vizyon
«İç müşterilerin ihtiyaç duydukları her türlü teknolojiyi bulup kendi başlarına yönetebilecekleri, yeniden kullanılabilir, esnek ve müşteri odaklı bir platform geliştirerek tüm HAVELSAN KKST yazılım geliştirme süreçlerini ortak bir çatı altında toplamak.»

Misyon
«Modern yazılım geliştirme ihtiyaçlarını göre en ileri Open Source, Cloud Native teknolojileri araştırmak, edindiğimiz tecrübe ile iç müşterilerimizin ve KKST Projelerinin yazılım ve donanımın  altyapılarının ihtiyaç analizini, tasarımını ve uygulamasını maliyet etkin şekilde yapmaktır.»
---
Dahili Geliştirici Platformu (IDP), bir platform ekibi tarafından daha önceden tanımlamış altyapı yığınlarını oluşturmak ve geliştiricinin bu yığından self-servis hizmet alabilmesini mümkün kılmak için oluşturulmuştur.  Bir IDP, temel yazılım geliştirme süreçlerini, geliştiricilerin bilişsel yükünü azaltacak şekilde birbirine bağlanmış birçok farklı teknoloji ve araçla bütünleştirir. Platform ekipleri, en iyi uygulamaları takip ederek platformlarını bir ürün olarak ele alır ve kullanıcı geri bildirimlerine dayalı olarak geliştirir, sürdürür ve sürekli iyileştirir.
---
IDP’nin Temel Amaçları
Açıklık ve Tahmin Edilebilirlik,
Standardizasyon ve Denetim,
Portföy grubu yazılım ekiplerinin ilgili alanlarına yoğunlaşmaları,
Otomasyon,
Verimlilik.
---
IDP 
Yazılım Kalite Hedefleri
Kalite standardı: ISO/IEC 25010:2011(E)
---
NATO Uyumluluğu
HAVELSAN IDP platformu ve geliştirmek için kazanılan teknolojik altyapı, NATO’nun NATO Architectural Framework (NAF) uyumlu Federated Mission Network (FMN) ve orta vadeli gelecek için ortaya koyduğu NATO Combat Cloud altyapılarıyla uyumludur.
---
NATO Future Core Services
2024 CWIX Future Core Services Roadmap Standards
NATO FMN Spiral 6 Uyumluluk İsterleri
OCI Distrubition Specification (Manifests, Digests etc.),
OCI Image Format Specification, 
OCI Runtime Specification,
Kubernetes API.
---
PAT Yaptıklarımız
Sanallaştırma Bağımsız Hale gelmek,
Sanal Router, Loadbalancer kullanımı,
Otomotize VM Provizyonlama,
CSI Uyumlu Volume Provizyonlama,
Yedekli Object Storage Backend Altyapısı,
eBPF CNI Network Altyapısı,
Key-Value Store,
Secret Store,
Otomatik Sertifika Provizyonlama Altyapısı,
Artifact, Image Repo Altyapısı,
Cloud Native Databases (MongoDB, Postgresql, vb.)
Cloud Native Middlewares,
Edge Computing K8S Cluster
---
Devam Edenler ve Yapılacaklar
Otomotize K8S Provizyonlama,
Media Server; Stream, Record and Replay,
Cloud Native CI/CD Altyapısı,
IDP Kullanıcı Arayüzü,
Veri Replikasyonu tasarım ve yönetim ekranları,
Loglama, Metric ve Monitöring Altyapısı,
Devsecops Altyapısı,
Kubernetes Security Altyapısı,
HAVELSAN Süreçlerine Uyumlu Raporlama,
Saha Kurulum altyapı, yazılım sürümü oluşturma ve kurma altyapısı
---

1 Design Decisions
1.1 Architectural Pattern
MATRA adopts a microservices architecture, which involves breaking down the application into smaller, self-contained services, each responsible for a distinct business capability. Unlike monolithic architectures, where all components are tightly coupled, microservices operate independently, with each service managing its own data and logic. This decoupling enables to develop different services concurrently, using the best-suited technologies for each. The architecture is inherently modular, allowing individual services to be deployed and scaled independently, which enhances the system’s ability to adapt to new requirements and scale according to demand. The microservices communicate with each other via APIs, often leveraging REST or messaging systems like Kafka for asynchronous communication, ensuring that they remain loosely coupled while maintaining the overall integrity of the system.
The decision to implement a microservices architecture in MATRA is driven by several strategic benefits that align with goals. First and foremost, microservices offer unparalleled scalability. Each service can be scaled independently based on its specific load, allowing the system to efficiently handle varying levels of demand without over-provisioning resources. This is particularly important for services like track and alarm management systems that may experience high traffic or need to process large volumes of data.  Additionally, the microservices approach significantly enhances system resilience. In a monolithic architecture, a failure in one component can cascade and bring down the entire system. In contrast, microservices are designed for fault isolation; if one service fails, it can be quickly addressed or replaced without affecting the overall system’s functionality. This improves uptime and reliability, which are critical for maintaining service continuity in production environments.
Another key advantage is the flexibility that microservices provide in terms of technology selection. Different services can be built using various programming languages, frameworks, or databases that are best suited to their specific needs.  Furthermore, microservices architecture supports rapid development and deployment cycles. With services being independently deployable, MATRA can implement continuous integration and continuous deployment (CI/CD) practices more effectively, leading to faster delivery of new features and bug fixes. This agility is essential for MATRA to quickly respond to evolving business requirements and customer feedback. 
While the microservices architecture offers numerous advantages, it also introduces significant challenges that need to be carefully managed. One of the primary challenges is the complexity of inter-service communication. In a distributed system like microservices, ensuring reliable and efficient communication between services is crucial. This can be particularly challenging when services need to communicate across different networks or data centers. To mitigate this, MATRA employs robust API gateways and DNS service discovery mechanisms that manage the routing of requests and handle load balancing across services. Additionally, to address the potential latency and network failures inherent in distributed systems, MATRA utilizes asynchronous communication patterns through message brokers like Kafka. This decouples services, allowing them to operate independently while ensuring that messages are reliably delivered even in the face of network issues. 
Another challenge is maintaining data consistency across distributed services, especially when using both an object database like MongoDB and a relational database like PostgreSQL. In a microservices architecture, MongoDB is often used for handling large volumes of unstructured or semi-structured data, such as track data, alarm logs, or sensor data, due to its flexible schema and horizontal scalability. PostgreSQL, on the other hand, is preferred for services that require strong data integrity, complex queries and transactional support such as authentication management. The challenge arises in ensuring that the data across these different databases remains consistent. To address this, MATRA implements eventual consistency models, where updates to data are propagated asynchronously across services, ensuring that all parts of the system eventually reach a consistent state. In cases where stronger consistency is required, distributed transaction patterns, such as the Saga pattern, are employed to coordinate complex operations across multiple services. This approach leverages the strengths of both MongoDB and PostgreSQL, using each where it fits best within the microservices architecture.
The complexity of managing and deploying multiple microservices also poses a significant challenge. Over a hundred services to manage, ensuring that each service is correctly deployed, monitored and maintained can be challenging. To achieve this, MATRA heavily relies on CI/CD pipelines that automate the building, testing and deployment processes. These pipelines are designed to handle the orchestration of deployments across multiple environments, ensuring that new code is safely and efficiently rolled out. Additionally, comprehensive monitoring and logging systems are implemented to provide visibility into the performance and health of each service, allowing system administrator to quickly identify and resolve issues before they impact users.
1.2 Bare-metal design
The bare-metal architecture refers to deploying software directly on physical servers without an intervening layer of virtualization. This approach allows direct access to hardware resources, such as CPU, memory and storage, enabling applications to achieve higher performance, lower latency and more predictable behavior.
In this architecture, each server runs a single operating system instance, in MATRA’s case Ubuntu Server with the latest LTS version and resources are allocated directly to applications, maximizing efficiency. Bare-metal environments are particularly well-suited for workloads that require consistent, high-performance computing, such as databases, real-time data processing and latency-sensitive applications.
Choosing bare-metal deployment over virtualized environments offers several advantages, especially in scenarios where performance, control and resource efficiency are critical. Key reasons for opting for a bare-metal design include:
    • Performance: Bare-metal servers eliminate the overhead introduced by hypervisors, leading to improved performance for compute-intensive applications. This is crucial in environments where every bit of processing power is needed, such as high-performance computing.
    • Resource Efficiency: By directly allocating hardware resources to applications, bare-metal servers ensure that all system resources are fully utilized without the inefficiencies often associated with virtualization layers.
    • Predictability: With no virtualization layer, applications run in a more predictable environment, which is essential for latency-sensitive applications. This predictability is key in Matra’s fusion and correlation services, where milliseconds can make a difference.
    • Security: Bare-metal environments reduce the attack surface compared to virtualized environments. With fewer layers of abstraction, there are fewer vulnerabilities for attackers to exploit, making it a more secure option for handling sensitive workloads.
    • Control and Customization: Bare-metal servers offer greater control over the hardware and software stack, allowing for more customized and optimized configurations tailored to specific workloads. This level of control is beneficial when using specialized hardware or optimizing specific performance aspects.
While the bare-metal design offers significant benefits, it also presents certain challenges, particularly in provisioning, scalability and management. In Matra, these challenges are effectively addressed through the use of Cluster API and Metal3.
    • Provisioning and Deployment Complexity: Traditionally, provisioning bare-metal servers is a time-consuming and complex task, requiring manual intervention at various stages. This challenge is mitigated because of Cluster API and Metal3, which automate the entire process, from server bootstrapping to Kubernetes deployment. These tools streamline the setup and ensure consistency across all servers, reducing the operational burden and potential for errors.
    • Lifecycle Management: Managing the lifecycle of bare-metal servers, including upgrades, patches and hardware maintenance, can be complex and prone to downtime. This challenge is mitigated because of the lifecycle management capabilities provided by Cluster API and Metal3. These tools facilitate seamless updates and maintenance tasks with minimal disruption, ensuring that the environment remains secure and up-to-date while maintaining high availability.
    • Scalability Concerns: Scaling bare-metal environments is more challenging than scaling virtualized environments. With Cluster API and Metal3, this issue is mitigated because of their ability to dynamically add or remove nodes from the cluster as application demands change. This ensures that the infrastructure can grow or shrink efficiently, without the manual overhead traditionally associated with bare-metal scaling.
By leveraging Cluster API and Metal3, we effectively address the challenges associated with bare-metal deployments, allowing us to achieve the performance and efficiency benefits of bare-metal while minimizing the complexities and risks traditionally involved.
1.3 Container Orchestration
MATRA employs a robust container orchestration strategy to efficiently manage and scale its complex microservices architecture. This approach is central to ensuring that the deployment, scaling and operation of containerized applications are automated, resilient and adaptable to changing demands. By leveraging container orchestration, MATRA achieves streamlined operations, improved resource utilization and enhanced system reliability.
The decision to adopt container orchestration arises from the need to handle a large number of interconnected services that must operate seamlessly together. Manual management of these services would be impractical and error-prone, especially as the system grows in size and complexity. Container orchestration provides an automated and declarative way to manage the lifecycle of containers, enabling developers and operators to focus on delivering value rather than managing infrastructure details.
Key Benefits and Features
Automated Deployment and Scaling:  Container orchestration allows for the automated deployment of services across a cluster of machines, ensuring that applications are launched consistently and efficiently. It also facilitates automatic scaling of services based on real-time demand, adding or removing instances to maintain optimal performance and resource usage.
Service Discovery and Load Balancing: Orchestration platforms provide built-in mechanisms for service discovery, enabling services to locate and communicate with each other dynamically. Integrated load balancing distributes incoming traffic across multiple service instances, ensuring high availability and responsiveness even under heavy load.
Self-Healing Capabilities: The orchestration system monitors the health of running services and can automatically recover from failures by restarting or replacing faulty instances. This self-healing feature ensures continuous availability and minimizes downtime without manual intervention.
Declarative Configuration Management: Desired states of the system, including configurations and deployment parameters, are defined declaratively. The orchestration platform continuously works to maintain this desired state, simplifying configuration management and enabling consistent environments across development, testing and production.
Efficient Resource Utilization: By intelligently scheduling containers based on resource requirements and availability, container orchestration ensures optimal utilization of compute, memory and storage resources. This leads to cost-effective operations and the ability to run workloads efficiently across diverse infrastructure setups.
Environment Consistency and Portability: Container orchestration abstracts underlying infrastructure differences, providing a consistent platform for deploying applications across various environments. This portability reduces complexity when migrating workloads between on-premises and cloud environments or across different cloud providers.
In designing the container orchestration strategy, several critical factors were considered:
Scalability: The system must support seamless scaling to accommodate growing workloads and user demands. The orchestration solution should handle both horizontal and vertical scaling efficiently.
Flexibility: Support for diverse workloads, including both stateless and stateful applications, is essential. The orchestration platform should provide mechanisms to manage persistent storage and stateful services effectively.
Security: Robust security features are necessary to protect sensitive data and ensure secure communication between services. The orchestration system should support integration with security tools and practices such as secret management, access controls and network policies.
Operational Simplicity: The platform should simplify complex operational tasks through automation and provide intuitive management interfaces and APIs. This reduces operational overhead and accelerates development and deployment cycles.
Ecosystem and Community Support: Choosing an orchestration solution with a strong ecosystem and active community ensures access to a wide range of tools, extensions and best practices, facilitating ongoing improvement and support.
Implementing container orchestration in MATRA  Kubernetes as the control plane that manages the desired state of the system, with worker nodes where application containers run. Kubernetes handles all aspects of the container lifecycle, including scheduling, deployment, scaling and health monitoring. Infrastructure as Code (IaC) principles and automation tools are utilized to provision and manage the Kubernetes infrastructure declaratively, ensuring reproducibility and ease of maintenance.
The adoption of container orchestration, with Kubernetes as the core platform, is a strategic decision that empowers MATRA to manage its microservices architecture effectively. Kubernetes manages the entire container lifecycle, including scheduling, deployment, scaling and health monitoring, while leveraging Infrastructure as Code (IaC) principles and automation tools to provision and maintain the infrastructure declaratively. This approach ensures reproducibility, ease of maintenance and brings automation, resilience and scalability to the forefront, enabling the system to adapt to evolving requirements while maintaining high performance and reliability.
By integrating these management tools with Kubernetes, MATRA ensures that clusters are managed efficiently, with minimal downtime and maximum reliability. This focus on automation and declarative management allows the system administrators to maintain a large and complex Kubernetes environment with greater ease and consistency.
1.4 Key Topics
1.4.1 Kubernetes
Kubernetes is the foundation of MATRA’s technology stack, seamlessly integrating with various other components to provide a cohesive and resilient environment for running microservices. Beyond its role in container orchestration, Kubernetes interacts with the broader infrastructure, enabling sophisticated management of networking, storage, security and observability.
Kubernetes offers support for both stateless (like microservices and micro-frontends) and stateful (like databases, message queues and object storages) applications, which is essential for MATRA’s diverse workload. The platform’s built-in service discovery and load-balancing mechanisms ensure that services can communicate reliably and that traffic is distributed evenly, preventing bottlenecks and ensuring high availability. 
Additionally, Kubernetes' support for declarative configuration and infrastructure as code practices aligns well with MATRA's goals of automation and consistency in deployment. This approach ensures that environments are easily reproducible, reducing configuration errors and enabling smoother transitions between development, testing and production environments.
Network complexity is managed in MATRA by leveraging Cilium as the Container Network Interface (CNI). Cilium is chosen primarily for its eBPF (Extended Berkeley Packet Filter) feature, which enables high-performance networking with deep visibility and control over network traffic. Additionally, Cilium’s BGP (Border Gateway Protocol) capability allows for efficient routing without the need for external load balancers, simplifying the network architecture while maintaining high availability and scalability. This approach not only enhances security and performance but also reduces the complexity typically associated with managing network traffic in a large-scale microservices environment like MATRA.
Implementing Kubernetes for container orchestration introduces several challenges, particularly in the management of persistent storage, especially for stateful applications. Kubernetes provides several options for persistent storage, but configuring and managing these options to ensure data consistency and availability across the cluster can be complex. In MATRA, Local Path Provisioner is utilized for local storage persistence, enabling quick and easy storage provisioning. Additionally, the DirectPV CSI driver is used for direct attached storage, providing efficient, low-latency storage directly on the nodes. For block storage, MATRA integrates S3 (Minio), offering scalable and durable storage for stateful workloads. By carefully selecting these storage solutions, MATRA ensures reliable and scalable options that meet the needs of the various services within the microservices architecture.
Monitoring and logging are critical components of managing a Kubernetes environment, especially in a microservices architecture. MATRA utilizes OpenTelemetry to provide comprehensive observability across all services. OpenTelemetry is chosen for its ability to offer a unified approach to collecting, processing and exporting telemetry data, including metrics, logs and traces. This integration ensures that the system administrator has real-time visibility into the performance and health of the system, enabling quick detection and resolution of issues. By standardizing on OpenTelemetry, MATRA simplifies the monitoring setup while ensuring that all telemetry data is consistently captured and processed, providing valuable insights into both the infrastructure and application layers.
Managing multiple Kubernetes clusters across different environments can become complex, especially as the number of microservices and nodes increases. To address this, MATRA utilizes Cluster API to manage Kubernetes clusters declaratively. Cluster API provides a standardized way to provision, upgrade and scale clusters, allowing for consistent cluster management across different infrastructure providers. This approach simplifies cluster life-cycle management, ensuring that clusters remain up-to-date and aligned with requirements. 
1.4.2 Container Network Interface
MATRA uses Cilium as the Container Network Interface (CNI) to enhance the networking capabilities within its Kubernetes cluster. Cilium’s deep integration with Kubernetes allows for seamless management of network policies, service discovery and load balancing within the cluster. It leverages Kubernetes’ native constructs, such as NetworkPolicy, to enforce security and manage traffic, ensuring that networking remains consistent and manageable as the cluster grows. This integration simplifies the deployment and management of network resources, aligning with MATRA’s goals of automation and consistency.
Cilium is chosen for its advanced features, particularly its use of eBPF (Extended Berkeley Packet Filter) for high-performance, secure and scalable networking. eBPF allows for deep visibility and control over network traffic at the kernel level and enables Cilium to dynamically insert code into the Linux kernel, optimizing packet processing and providing fine-grained network security policies. This capability is important for MATRA, where secure, efficient and highly performant networking is essential for managing communication between microservices.
Cilium also integrates BGP (Border Gateway Protocol) to facilitate advanced routing capabilities within the cluster. By leveraging BGP, MATRA can achieve high availability and efficient traffic routing without the need for external load balancers. This simplifies the network architecture, reduces complexity and enhances the overall resilience of the system. BGP allows the network to adapt quickly to changes, such as node failures or changes in network topology, ensuring that traffic is always directed to the most appropriate service endpoints.
MATRA achieves robust observability features by using Cilium, including detailed insights into network flows, security policy enforcement and performance metrics. These features are integrated with MATRA’s overall monitoring strategy, allowing the system administrator to track and analyze network performance in nearly real-time. This visibility is essential for identifying potential issues, optimizing network configurations and ensuring that the system operates efficiently under various conditions.
1.4.3 Software Defined Storage Solutions
MATRA uses a uses Software Defined Storage (SDS) strategy to manage its diverse storage needs efficiently. SDS abstracts the underlying storage hardware, allowing for dynamic allocation of resources based on application requirements, ensuring high performance, scalability and flexibility. Even within a bare-metal architecture, this approach ensures that applications and shared services interact with storage through abstracted layers rather than accessing physical disks directly, maintaining both flexibility and optimal performance.
Local Path Provisioner is used in MATRA to manage local storage within the Kubernetes environment, specifically for stateful sets like Kafka, MongoDB and PostgreSQL clusters. This setup provides reliable, node-attached storage for these stateful applications, reducing latency and improving performance by ensuring that data is stored and accessed locally on the nodes where the applications run. This is particularly effective for maintaining the performance of databases and other services that require consistent, low-latency access to data.
DirectPV, a CSI driver, is used for the Minio (S3) cluster in MATRA. It provides direct access to attached storage devices, bypassing traditional networked storage overhead. This setup is ideal for handling the high-performance demands of the S3 cluster, allowing for efficient data storage and retrieval, especially when dealing with distributed data environments. DirectPV ensures that the S3 cluster operates with the low latency and high throughput required by MATRA’s object storage needs.
Minio serves as the S3-compatible object storage solution of MATRA, providing scalable, high-performance storage. The key use of Minio is mounting S3 buckets to pods, enabling distributed data storage and access. This allows applications can easily access and store distributed data across the cluster if needed.
1.4.4 Ingress, API Gateway and Reverse Proxy
MATRA utilizes Traefik as the Kubernetes ingress controller, API gateway and reverse proxy. Traefik is a modern, cloud-native solution that is highly compatible with Kubernetes, making it an ideal choice for managing ingress traffic in a microservices architecture.
As the ingress controller, Traefik manages external access to services running inside the Kubernetes cluster. It routes incoming requests to the appropriate backend services based on rules defined in Kubernetes Ingress resources. Traefik’s dynamic configuration capabilities allow it to automatically detect new services and update its routing rules without requiring manual intervention, which is essential for the nature of microservices environments.
Traefik also serves as the API gateway for MATRA, handling tasks such as load balancing, SSL termination and API authentication and authorization. By centralizing these functions, Traefik simplifies the management of API traffic and enhances the security of the system by providing a single point of entry for external API calls. This setup ensures that API requests are efficiently routed, secured with TLS encryption, authenticated and authorized before reaching the backend services.
As a reverse proxy, Traefik provides essential features such as URL rewriting, request redirection and traffic splitting. These capabilities are crucial for MATRA, where traffic needs to be intelligently routed based on various factors, such as the user’s location, the type of request, or the current load on the backend services. Traefik’s ability to perform these tasks dynamically, based on real-time traffic patterns, helps to optimize resource utilization and maintain high performance across the system.
Traefik is designed to scale effortlessly with the infrastructure, making it a perfect fit for MATRA’s microservices architecture. It can handle a high volume of requests and scale in and out as the load increases or decreases. Additionally, Traefik integrates well with Kubernetes, allowing it to leverage Kubernetes’ native features, such as service discovery and load balancing to ensure that traffic is always routed to the most appropriate service instance.
1.4.5 Centralized Configuration
MATRA adopts a centralized configuration management strategy to ensure consistency, reliability and efficient management of configuration data across its microservices architecture.
MATRA uses git version control for configuration storage to provide robust version control, allowing the track changes to configuration files over time. This capability is crucial for maintaining a history of changes, rolling back to previous versions if necessary and auditing modifications for compliance. By storing all configuration data in a git repository, MATRA ensures that every microservice pulls its configuration from a centralized, consistent source, eliminating the risk of different services using outdated or inconsistent configurations.
The centralized configuration server, providing configuration data to all microservices. This ensures that each service has access to the latest configuration settings, regardless of where it is deployed. One of the key advantages of config server is its ability to push configuration changes to services dynamically, without requiring redeployment. This feature is particularly important in MATRA, where configuration changes may need to be applied quickly to adapt to new requirements or resolve issues. Additionally, config server supports environment-specific configurations, allowing MATRA to tailor settings for different environments. This ensures that each microservice operates with the appropriate settings for its context.
Centralized configuration management using git and config server provides several significant benefits. It ensures that all microservices operate with the same configuration data, reducing the likelihood of errors and inconsistencies. It facilitates the management of configuration across a large and growing number of microservices, allowing the system to scale efficiently. Furthermore, by centralizing configuration data in Git, access can be tightly controlled and sensitive information can be encrypted, reducing the risk of unauthorized access or exposure.
MATRA ensures that configuration management is efficient, reliable and adaptable to the evolving needs of the operation, providing a solid foundation for the continued growth and maintenance of the system.
1.4.6 Middlewares
Middleware technologies are essential for communication, data processing and integration between microservices and external services. They act as the backbone that supports data flow, message handling and efficient caching throughout the system. Middlewares enable the decoupling of services, allowing each component to operate independently while maintaining seamless interaction with other parts of the architecture. Solutions like Redis and Kafka are key for managing the complexities of a microservices environment, providing the necessary infrastructure for scalability, performance optimization and data processing. These technologies are chosen for their reliability, scalability and their ability to handle specific tasks within the microservices ecosystem.
1.4.6.1 Caching
In MATRA Redis is an important element that improves the efficiency and responsiveness of the entire system. It focuses on optimizing data access, enhancing performance and enabling effective data management.
Backend services use Redis to store frequently accessed data in RAM, which significantly reduces the number of direct queries sent to the underlying databases. By caching this data, MATRA ensures that user requests to the application interface are served quickly, providing near-instantaneous responses. This approach minimizes the load on the primary databases, allowing them to focus on handling writes and more complex queries. As a result, network traffic is reduced and the overall system performance is enhanced, ensuring a smoother user experience.
MATRA uses Redis for handling geospatial queries, which are important for applications that involve mapping and geographic data processing. The geospatial indexing feature in Redis allows for the rapid execution of geospatial queries, enabling faster generation and classification of map data. This capability is very important for MATRA, where users rely on the system for timely and accurate geographical information. By processing geospatial queries within Redis, backend services can deliver results more efficiently, reducing latency and improving the responsiveness of map-related features.
Redis is essential to scaling Web Sockets in MATRA, particularly through its pub/sub pattern. web sockets are used as a communication protocol for real-time data transmission, such as sending alarms or notifications. Redis acts as a central message broker, ensuring that messages published by users are distributed across all connected servers. The pub/sub pattern allows Redis to broadcast these messages to all subscribers, ensuring consistent communication across multiple servers. This is vital for maintaining the reliability and speed of real-time data transmission, which is critical for applications that depend on instantaneous updates, such as alarm systems and live notifications.
By integrating Redis into the technology stack, MATRA significantly optimizes performance and reduces latency across various aspects of the application. Whether it’s providing rapid access to frequently used data, speeding up geospatial queries, or enabling real-time communication through web sockets. 
1.4.6.2 Messaging
Kafka is a core communication component in MATRA, playing an important role in managing messaging between microservices and other systems. Kafka operates within the Kubernetes environment, leveraging the consensus mechanism to maintain a high-availability (HA) cluster, ensuring that messaging services are reliable and fault-tolerant.
Kafka is used primarily for asynchronous communication between microservices, allowing them to interact without direct dependency on each other’s availability. This decoupling is essential for maintaining the resilience and scalability of the microservices architecture, as it enables services to produce and consume messages at their own pace, ensuring that operations continue even if some services are temporarily unavailable. Kafka’s capability for handling high-throughput and low-latency messaging also makes it ideal for data transfer and processing. Whether the data originates from remote data centers or internal, Kafka’s fault-tolerant cluster ensures that messages are securely transmitted and processed in nearly real-time. This capability is crucial for applications that require immediate data process, analysis and response, such as track and alarm management systems.
Another critical use of Kafka in MATRA is to manage communication between sensor processing software located in SSLs and the CCC. Kafka handles the secure and complete transmission of data collected in SSLs to the CCC. This asynchronous communication method ensures that even if there are delays or interruptions in the network, the data is eventually delivered and processed without loss, maintaining the integrity of the operations that depend on this sensor data.
The Kafka cluster in MATRA is designed to meet the demands of the system as the volume of messages increases. Kafka can handle the increased load without degradation in performance. Kafka’s built-in fault tolerance, through replication, guarantees that even in the event of broker failures, the system remains operational and data is not lost.
Kafka also plays a key role in maintaining the security and integrity of the data being transmitted. By using encryption and secure protocols data is protected during transit. This is particularly important for the sensitive operational data being transmitted between the SSLs and CCC, where the integrity and confidentiality of the data must be maintained at all times.
1.4.7 Databases and Object Stores
MATRA relies on a combination of databases and object stores to manage and store diverse types of data efficiently. These technologies are chosen to meet the specific needs of different services within the microservices architecture, ensuring that data is stored, accessed and processed in the most effective way possible. The use of both relational and NoSQL databases, along with scalable object storage, provides the flexibility and performance required to handle structured, semi-structured and unstructured data. This allows MATRA to maintain data consistency, support complex queries and manage large volumes of data while ensuring scalability and high availability across the entire system.
1.4.7.1 NoSQL Database
MongoDB is a central component in MATRA’s data architecture, serving as the NoSQL database responsible for storing and managing a vast array of operational data. This includes critical information such as track data, alarms, areas and user profiles. The decision to use MongoDB is driven by its ability to meet the demanding performance, scalability and flexibility requirements necessary for processing real-time data from various sources, such as Radar, AIS and RDFs.
MongoDB’s flexible schema is particularly well-suited for MATRA, which needs to accommodate a wide variety of data types, ranging from structured user information to unstructured sensor data. The document-oriented model allows the storage of complex data structures within a single document, making it easier to manage and query the diverse datasets that the system ingests. This flexibility is crucial for handling the rapidly evolving nature of operational data, which may vary significantly in structure and content.
MATRA’s integration with nearly real-time data sources requires a database that can scale horizontally to manage large volumes of continuously incoming data. High availability is ensured through MongoDB’s replication features, which provide data redundancy and resilience, protecting against hardware failures and ensuring that critical data remains accessible.
Requirements to integrate with nearly real-time data sources and performance is also a critical factor in the choice of MongoDB. The database is optimized for high-speed read and write operations, making it ideal for processing and storing nearly real-time operational data such as radar tracks. This ensures that users receive the most accurate and up-to-date maritime picture with minimal delay. MongoDB’s ability to handle high throughput is essential for maintaining the system’s responsiveness, even as the volume of incoming data increases.
MongoDB’s ability to handle the diverse, high-volume and real-time data requirements of MATRA, coupled with its performance, scalability and security features, makes it the ideal choice for managing the system’s operational data. Its flexibility and powerful querying capabilities ensure that MATRA can deliver timely and accurate information to its users, supporting critical decision-making processes.
1.4.7.2 RDBMS
PostgreSQL is used in MATRA to store data that does not require frequent read and write operations but demands high data integrity. This makes it an ideal choice for managing critical information where consistency and accuracy are significant. For instance, ORY Hydra (Authentication Provider) utilizes PostgreSQL to store user authentication data, ensuring that the system maintains secure and reliable access controls. PostgreSQL’s robust transactional capabilities and ACID compliance make it well-suited for this purpose, ensuring that all operations on the data are processed reliably and without compromise.
1.4.7.3 File and Object Storage
Matra uses Minio as the S3-compatible storage solution for file and object storage. Minio is chosen for its ability to provide scalable and high-performance storage that integrates seamlessly with the rest of the system.  Minio’s ability to handle large volumes of data makes it an ideal choice for this purpose, ensuring that files and objects are stored securely and can be accessed quickly when needed.
Minio (S3) is essential to the process of recording EO video data, allowing the system to store video files efficiently. These recordings are important for MATRA, where historical video data is needed for replay. The capability to replay these videos from Minio via a media server allows users to review critical recordings as needed, without delay.
To enhance the resilience of video data storage, Minio’s S3 bucket-to-bucket replication feature is used to replicate EO video records from the SSLs to the CCC. This replication ensures that even if network problems or outages occur, the video data is securely transferred and preserved at the CCC. This redundancy is important for maintaining data integrity and availability, ensuring that the operational data remains consistent across locations and is accessible for future use.
Minio’s scalability is another reason it was chosen for MATRA. It can handle the growing volume of data generated by EO video recordings and other operational objects without performance degradation. This flexibility allows MATRA to expand its storage capacity as needed, supporting the continuous growth of data over time.
By utilizing Minio for S3-compatible storage, MATRA covers its file and object storage needs with a solution that prioritizes scalability, resilience and reliable access to critical data.
1.4.8 Centralized Authentication and Authorization
In MATRA, centralized authentication and authorization is a fundamental design decision within the microservices architecture, ensuring that access to system resources is consistently and securely managed across all services. This approach simplifies identity management, enhances security and reduces the complexity associated with handling authentication and authorization in a distributed environment.
ORY Hydra is used as the authentication provider because of its robust support for OpenID Connect and OAuth 2.0 protocols, which are essential for securely managing user identities across multiple services. It is well-suited for the microservices architecture as it provides a scalable and centralized solution for user authentication, ensuring that only users with valid credentials can access the system. Works  seamlessly with the KAPI user management system to verify that user login information. This integration allows ORY Hydra to efficiently authenticate users, ensuring that only users with valid credentials can access the system, while centralizing the authentication process and minimizing potential security vulnerabilities.
OPA (Open Policy Agent) is utilized for authorization in MATRA because of its ability to enforce access control policies centrally, ensuring that all authorization decisions are made consistently across the entire system. OPA allows for the definition and management of complex authorization policies in a single place, which are then uniformly applied to all microservices. This approach simplifies policy management, enhances security by eliminating the need for each service to manage its own authorization logic and ensures that access to resources is tightly controlled based on user roles, permissions and other criteria.
Traefik’s middlewares are employed in MATRA  because of their ability to seamlessly integrate with ORY Hydra and OPA, enabling centralized authentication and authorization checks at the API gateway level. Traefik acts as the entry point for all incoming requests, where its middleware architecture intercepts these requests and applies the necessary identity checks before they are routed to the appropriate service. This integration ensures that security policies are enforced consistently across the system, enhancing both the security and efficiency of the overall architecture.
By adopting a centralized approach to authentication and authorization using ORY Hydra, OPA and Traefik’s middlewares, MATRA ensures that user access is tightly controlled, consistent and secure across the entire microservices ecosystem. This design decision aligns with best practices in microservices architecture, providing a strong foundation for protecting sensitive resources and data.

1.4.9 Centralized Certificate and Secret Management
Centralized management of certificates and secrets is important for maintaining security, consistency and efficiency in MATRA. By handling these critical elements from a central point, the system can better protect sensitive data and ensure that security practices are uniform across all parts of the infrastructure.
One of the main benefits of centralizing certificate and secret management is improved security. By enforcing consistent security standards across the entire system, the chances of security gaps or vulnerabilities are significantly reduced. This unified approach to encryption, access control and monitoring makes it easier to safeguard sensitive information.
MATRA uses Vault for its robust security features, including its ability to securely manage certificates through its PKI store and sensitive data through its key-value store, all within a centralized, scalable and flexible platform.. The PKI store allows for the secure issuance and management of certificates, which are critical for securing communication between services. The KV store securely manages sensitive information like database passwords and API keys, ensuring that only authorized components have access to this data.
Centralized management also ensures consistency across all environments. This uniformity helps prevent configuration errors that could lead to security issues or system failures. By relying on a single, central source for all certificates and secrets, MATRA ensures that the system operates smoothly and reliably.
In addition, centralizing the management of certificates and secrets simplifies administrative tasks. Instead of dealing with multiple systems or tools, everything can be managed from one place. This simplification makes it easier to deploy new services, update existing ones and quickly resolve issues when they arise. Meeting regulatory requirements is also easier with centralized management. It provides a clear record of all actions related to certificates and secrets, which helps MATRA demonstrate compliance with regulations and respond swiftly to audits or security reviews.
Centralizing certificates and secrets reduces the risk of human error. Automation can be more easily implemented, lowering the chances of mistakes like misconfigurations or overlooked updates. Consistent application of critical updates, such as renewing certificates, reduces the risk of outages caused by expired certificates. This approach also strengthens the protection of sensitive data, simplifies management tasks and reduces the risk of errors or vulnerabilities, providing a strong foundation for the ongoing security and reliability of the system.

1.4.10 Centralized Monitoring, Tracing and Logging
In MATRA, centralized monitoring, tracing and logging are vital because they provide a unified view of the system's health and performance, which is crucial for managing the complexity inherent in microservices architectures. In such a setup, services are distributed, making it challenging to track issues, performance bottlenecks and dependencies between services without a centralized observability solution.
Centralized monitoring allows for real-time tracking of system metrics, ensuring that performance issues can be identified and addressed promptly. Tracing provides insights into the flow of requests across services, making it easier to diagnose problems and optimize service interactions. Centralized logging aggregates logs from all services, enabling comprehensive troubleshooting and post-incident analysis. 
MATRA uses OpenTelemetry to cover all aspects of centralized monitoring, tracing and logging. OpenTelemetry serves as the backbone of the observability strategy, providing comprehensive capabilities for collecting metrics, logs and traces from across the entire system. By using OpenTelemetry, MATRA ensures that all services are consistently monitored, traced and logged, allowing for a unified and seamless approach to observability in a complex microservices environment. This integration ensures that performance issues can be quickly identified, service interactions are clearly understood and logs are readily available for troubleshooting and analysis, all within a single cohesive framework.
Metrics, traces and log visualization provides a clear and comprehensive understanding of the system's health, performance and behavior. Visualizing metrics allows the system administrator to monitor key performance indicators and resource usage, making it easier to identify potential issues before they impact the system. This proactive approach helps maintain optimal performance and avoid bottlenecks.
Visualizing traces is crucial for understanding how requests move through the various services in the system. This insight is essential for diagnosing issues, optimizing service interactions and ensuring that the system functions smoothly as a whole. By seeing how different components interact, system administrators can quickly identify and resolve any inefficiencies or errors in the request flow.
Log visualization is used to facilitate thorough troubleshooting and post-incident analysis. By correlating logs with metrics and traces, the system administrator can get a complete picture of what happened during an incident, enabling faster and more accurate issue resolution. This comprehensive approach ensures that all aspects of the system are monitored and understood, leading to higher reliability, quicker response times and improved overall system performance in MATRA.
Without centralized monitoring, tracing and logging, MATRA would face significant challenges in maintaining reliability, diagnosing issues and ensuring that all services operate seamlessly together. These capabilities are essential for achieving the high levels of performance, reliability and availability required in a microservices environment.
1.4.11 Continuous Deployment
Continuous Deployment (CD) is an important aspect of MATRA’s update management strategy, ensuring that new features, updates and patches are delivered rapidly and reliably. MATRA utilizes ArgoCD to manage and automate the deployment of both microservices and shared services across Kubernetes clusters.
ArgoCD is used to automate the deployment process by monitoring Git repositories for changes and synchronizing the state of both microservices and shared services with the desired configurations. Gitea serves as the Git server, hosting the repositories that ArgoCD monitors. For shared services like Kafka, MongoDB and PostgreSQL, Helm charts are used within ArgoCD to manage configurations and dependencies. This combination ensures that all services, whether microservices or shared services are deployed in a consistent, standardized and automated manner, reducing the likelihood of errors and enhancing overall system reliability.
Nexus serves as the central repository for all container images used in MATRA, storing images for both microservices and shared services. This centralized management ensures that images are securely stored, versioned and easily managed, supporting smooth updates and rollbacks.
MATRA supports various update strategies, including rolling updates, blue-green deployments and canary releases. This flexibility allows to select the most appropriate strategy for each scenario, ensuring that updates are applied without disrupting ongoing operations.
By incorporating ArgoCD, Helm, Gitea and Nexus into the continuous deployment pipeline, MATRA establishes a reliable and flexible framework for delivering updates efficiently, while maintaining high system availability and minimizing deployment-related risks.
1.5 High Availability and Scalability
High availability (HA) and scalability are fundamental requirements in MATRA to ensure that the system remains resilient and performs efficiently under varying loads. MATRA uses a combination of technologies and strategies to achieve these goals, ensuring that both the infrastructure and applications can handle failures and scale according to demand.
MATRA is designed with high availability in mind, utilizing redundancy, failover mechanisms and distributed architectures. Key services like Kafka, MongoDB and PostgreSQL are deployed in clustered configurations within Kubernetes to ensure they remain operational even if individual nodes fail. For instance, Kafka operates as a high-availability (HA) cluster using the Kraft consensus mechanism, which allows it to continue functioning even if some brokers go offline. Similarly, MongoDB and PostgreSQL are configured with replication and failover capabilities and managed within Kubernetes, ensuring that data remains accessible and consistent across the system. 
Kubernetes itself contributes to high availability by managing the distribution of workloads across multiple nodes, automatically restarting failed containers and ensuring that critical services are always running. The use of Kubernetes ensures that the system can recover quickly from failures, maintaining service continuity with minimal disruption.
Kubernetes also facilitates scaling by adjusting the resources allocated to containers, such as CPU and memory, based on the current demands of the application. This dynamic resource management allows the system to optimize performance and efficiency, scaling resources up or down as needed.  Shared services like Kafka and PostgreSQL are designed to scale by adding more nodes to their respective clusters within Kubernetes, distributing the load more evenly and preventing any single node from becoming a bottleneck.
MATRA also uses Cilium’s BGP integration to enhance network availability. By using BGP, the network can adapt quickly to changes in topology, rerouting traffic as necessary without relying on external load balancers. This reduces single points of failure and ensures that the network remains resilient even during outages or hardware failures. 
Traefik, the API gateway and reverse proxy, is responsible for distributing traffic across multiple instances of services, ensuring that no single instance is overwhelmed. This load balancing is critical for maintaining performance and availability as traffic fluctuates. Traefik’s integration with Kubernetes allows it to manage traffic dynamically, responding quickly to changes in load and ensuring that services are always available and responsive.
By integrating Kubernetes with these strategies and technologies, MATRA ensures that its infrastructure is both highly available and scalable, capable of handling failures gracefully and adapting to changing demands efficiently. This robust design helps maintain continuous service availability and optimal performance, even under challenging conditions.
1.6 Security Considerations
Security is a critical concern in MATRA, given the nature of the system and the sensitivity of the data it handles. MATRA uses a multi-layered security approach to protect its infrastructure, applications and data from unauthorized access, breaches and other security threats. This comprehensive strategy includes robust identity and access management, secure communication channels, data encryption and continuous monitoring to consistently apply security best practices.
MATRA uses a combination of ORY Hydra and OPA (Open Policy Agent) to manage user authentication and authorization. ORY Hydra handles user authentication, ensuring that only verified users can access the system. OPA is responsible for enforcing authorization policies and determining which resources a user is allowed to access based on predefined rules. This centralized approach to identity and access management reduces the risk of unauthorized access and ensures that security policies are enforced consistently across all services.
To protect data in transit, MATRA uses SSL/TLS encryption for all communications between services and external entities. Certificates are managed by Vault, which acts as a certificate authority (CA), issuing and renewing certificates as needed. Cert-Manager, running within the Kubernetes cluster, automates the certificate management process, ensuring that all communications are securely encrypted without the need for manual intervention.
Data at rest is encrypted using Vault’s capabilities integrated with MATRA’s resources. Sensitive data, such as database passwords and API keys, is stored in Vault’s key-value (KV) stores, with access tightly controlled and audited. By encrypting data both at rest and in transit, MATRA minimizes the risk of data breaches and ensures that sensitive information is protected from unauthorized access.
By implementing these comprehensive security measures, MATRA ensures that its infrastructure, applications and data are protected against a wide range of security threats. This multi-layered approach provides a strong foundation for maintaining the integrity, confidentiality and availability of the system, even in the face of evolving security challenges.
1.7 Data Management Strategy
The data management strategy in MATRA is designed to ensure the efficient handling, storage and retrieval of data across the system. Given the diversity of data types, ranging from operational data in MongoDB to relational data in PostgreSQL and large volumes of object data in S3, the strategy focuses on ensuring data integrity, availability and scalability while maintaining performance.
Data in MATRA is categorized based on its nature and usage requirements. Operational data, such as real-time track, alarm and user information, is stored in MongoDB due to its flexible schema and ability to handle high volumes of unstructured data. Relational data, which requires high integrity but less frequent updates, such as user authentication data managed by ORY Hydra, is stored in PostgreSQL. Large files and object data, such as media files, are stored in S3 (Minio), which provides scalable storage with support for bucket-to-bucket replication to ensure data redundancy and availability.
Ensuring data consistency and integrity is a core component of the data management strategy. MongoDB’s replication capability ensures that operational data is consistent across the cluster, even as it scales. PostgreSQL’s ACID-compliant transactions guarantee that static data remains accurate and reliable. Regular backups and replication strategies are employed to safeguard data, ensuring that it can be restored in the event of corruption or loss.
Access to data is tightly controlled and secured using encryption mechanisms provided by Vault. Accessing data at rest in MongoDB, PostgreSQL and S3 is protected from unauthorized access. Data in transit is protected using SSL/TLS encryption, ensuring that it cannot be intercepted or altered during transmission.
The data management strategy is designed to support the scalability needs of MATRA. MongoDB’s ability to scale horizontally allows it to handle growing volumes of operational data without performance degradation. S3’s scalable object storage is used for managing large volumes of data, with the flexibility to expand as needed. PostgreSQL’s performance is maintained through careful indexing and query optimization, ensuring that even as the data grows, the system remains responsive.
MATRA implements data retention policies that comply with operational and regulatory requirements. Data is retained for the necessary duration and is then securely deleted or archived. Following data protection and privacy standards is ensured through regular audits and by applying industry best practices.
This data management strategy ensures that MATRA can efficiently manage its diverse data needs, maintaining high levels of data integrity, availability and security, while also being scalable to accommodate future growth.
2 Architectural Design
2.1 System Overview
The System Overview section provides a high-level description of the architecture and key components of MATRA. This diagram outlines how the system is structured, the primary technologies used and how different components interact to deliver a cohesive and efficient solution. It serves as a foundational understanding of the overall system design, guiding the detailed exploration of specific areas in subsequent sections.


2.2 Bare Metal Architecture
As part of the SCOMAR, a microservices-based software architecture will be implemented. Applications will run on containers, thereby ensuring a flexible, accessible and scalable infrastructure. Utilizing Kubernetes on bare metal, which forms the basis of this infrastructure, offers significant advantages in terms of performance, security and customizability. Bare metal Kubernetes allows the full potential of the hardware to be utilized by eliminating the virtualization layer, enabling applications to operate with lower latency and higher performance. Additionally, it will eradicate the sluggishness associated with traditional monolithic deployment models.
Therefore, the preferred bare metal Kubernetes deployment model for the SCOMAR will play a crucial role in ensuring the project's success by providing the flexibility and scalability required by microservices architecture.

SCOMAR APP and Record Bare Metal Kubernetes Cluster
The SCOMAR App and Record Cluster will consist of a total of four physical servers. Each of these servers will operate as a Worker Node and the SCOMAR and Video Recorder application services will be distributed across all the servers. This arrangement not only distributes the workload among the servers but also ensures that no service disruption occurs in case of a physical server failure.

SCOMAR App and Record Kubernetes Cluster Physical Network Design Model
Each node will be equipped with a dual 25 Gbit/s port network card. The connections between the cards on the nodes and the network switches will be configured redundantly. The ports on the nodes will be configured with Link Aggregation Control Protocol (LACP) to ensure redundancy. High-speed 25 Gbit/s ports have been specifically chosen to facilitate the expected data synchronization among the Kubernetes cluster nodes.

SCOMAR App and Record Kubernetes Cluster Storage Design Model
All nodes in the SCOMAR App and Record Cluster will have disks of identical number, capacity and speed. For the operating system, there will be two 400 GB SSDs configured in RAID 1 for protection. For container configuration files and application data, there will be three 7.6 TB NVMe disks configured in RAID 5 for protection. Additionally, each node will be equipped with three 7.6 TB NVMe disks dedicated to video recording. Utilizing erasure coding protection, data will be distributed across these disks on all servers. This setup will provide a total of 40 TB object storage cluster. The cluster, comprising four nodes and twelve disks, will be capable of operating continuously even in the event of up to two node and six disk failures.

DMZ External Integration Bare Metal Kubernetes Cluster
The DMZ External Integration Cluster will consist of a total of three physical servers. Each of these servers will function as a Worker Node and the external integration application services will be distributed across all the servers. This configuration not only balances the workload among the servers but also ensures uninterrupted service in the event of a physical server failure.

DMZ External Integration Physical Network Design Model
Each node will be equipped with a dual 1 Gbit/s port network card. The connections between the cards on the nodes and the network switches will be configured redundantly. The ports on the nodes will be configured with Link Aggregation Control Protocol (LACP) to ensure redundancy.

DMZ External Integration Kubernetes Cluster Storage Design Model
All nodes in the DMZ External Integration Cluster will have disks with identical number, capacity and speed. For the operating system, there will be two 400 GB SSDs configured in RAID 1 for protection. For container configuration files and application data, there will be three 1.92 TB NVMe disks configured in RAID 5 for protection.

SCOMAR App and Record Local Bare Metal Kubernetes Single Node
The SCOMAR App and Record Local Node will consist of a single physical server. This server will function as a Worker Node, running the SCOMAR and Video Recorder application services on this server.

SCOMAR App and Record Local Node Physical Network Design Model
The SCOMAR App and Record Local Node will be equipped with a dual 1 Gbit/s port network card. The connections between the card on the node and the network switch will be configured redundantly. The ports on the node will be configured with Link Aggregation Control Protocol (LACP) to ensure redundancy.

SCOMAR App and Record Local Node Storage Design Model
The SCOMAR App and Record Local Node will have two 400 GB SSDs configured in RAID 1 for the operating system. For container configuration files and application data, there will be three 1.92 TB NVMe disks configured in RAID 5. Additionally, the node will be equipped with four 1.92 TB NVMe disks dedicated to video recording, with data protection ensured through erasure coding technology.

2.3 Microservices Architecture
The microservices architecture of MATRA is designed to maximize modularity, scalability and maintainability. Each microservice or group of microservices is built to handle a specific business function, which allows for independent deployment and scaling. This architectural approach enables the system to be more resilient, as individual services can be updated or replaced without impacting the overall system.
From an architectural standpoint, MATRA's microservices design embodies several key principles. It prioritizes loose coupling and high cohesion, with each microservice encapsulating a well-defined business capability. 
The system embraces polyglot persistence, allowing each service to choose the most suitable data store for its needs. While this promotes flexibility, it also necessitates careful management of data consistency across the distributed system. The architecture leverages both synchronous (RESTful APIs) and asynchronous (Kafka) communication patterns, enabling efficient and decoupled interactions between services.
MATRA utilizes Kubernetes for orchestration, facilitating seamless scaling and management of microservices. Horizontal scaling allows for efficient handling of fluctuating workloads by adding or removing service instances.

To mitigate the inherent complexity of distributed systems, MATRA employs an API gateway to provide a unified entry point for clients. This simplifies client interactions and centralizes concerns such as authentication and rate limiting. Monitoring and observability are paramount in this architecture, with distributed tracing and centralized logging playing pivotal roles in understanding system behavior and troubleshooting issues.
Zooming in further on the architectural view of MATRA's microservices system, we observe a layered approach that enhances modularity and separation of concerns. At the edge, an API Gateway acts as the system's front door, handling routing, authentication and request transformation. This layer decouples clients from the internal microservices, providing a consistent interface while enabling internal services to evolve independently.
Within the system, services communicate through a combination of synchronous and asynchronous patterns. RESTful APIs enable direct, real-time interactions where immediate responses are crucial. Meanwhile, Kafka based event streaming enables loose coupling, allowing services to react to events and changes without direct dependencies. This event-driven architecture contributes to scalability and resilience, as services can process events at their own pace.
Data management in MATRA is decentralized, with each microservice potentially owning its dedicated database or collection. This approach aligns with the principle of service autonomy but introduces challenges related to data consistency and integration. Techniques like event sourcing or the Saga pattern might be employed to address these concerns, ensuring data integrity across the distributed system.
Observability is crucial in this complex landscape. MATRA incorporates distributed tracing to follow requests across services, providing insights into system behavior and performance bottlenecks. Centralized logging aggregates logs from various services, facilitating troubleshooting and monitoring.
From a deployment perspective, Kubernetes container orchestration enable efficient packaging and management of microservices. This approach promotes portability, scalability and resilience, allowing MATRA to adapt to changing workloads and requirements.
Overall, MATRA's microservices architecture represents a strategic choice for achieving agility, scalability and maintainability. While it comes with challenges such as increased operational complexity and distributed data management, the benefits in terms of flexibility, fault isolation and independent scaling make it a compelling approach for modern application development.
2.4 Cluster Architecture
MATRA is designed as a distributed, microservices-based system deployed on an on-premise Kubernetes cluster. The architecture is modular, with each component serving a specific function within the system, ensuring flexibility, scalability and maintainability.


At a high level, the cluster architecture of cluster consists of:
    • Container Orchestration Layer: Kubernetes is the foundation of MATRA's architecture, responsible for orchestrating the deployment, scaling and management of containerized applications. Kubernetes automates the management of resources across the cluster, ensuring efficient utilization and high availability of services.
    • Networking Layer: Cilium is used as the CNI for networking, providing secure and efficient traffic management within the Kubernetes cluster. Traefik acts as the ingress controller, managing incoming traffic and routing it to the appropriate services.
    • Data Storage and Persistence Layer: This layer includes MongoDB for operational data that requires flexible schema and high scalability, PostgreSQL for data that demands strong consistency and integrity and Minio (S3) for storing large video files from EO cameras. The media server integrates with this layer to manage live streaming and playback of video content.
    • Business Layer: A collection of loosely coupled services, each responsible for a specific business function. These services communicate with each other using REST for synchronous calls and Kafka for asynchronous messaging, enabling efficient data exchange and processing.
    • Security Layer: Security is a critical aspect of MATRA, integrated throughout the architecture. ORY Hydra and OPA are used for identity and access management, ensuring that only authorized users and services can access the system. Vault is employed for managing secrets and certificates, providing encrypted storage and secure distribution of sensitive information. The system also implements network policies and access controls to prevent unauthorized access and ensure data protection.
    • Observability and Monitoring Layer: OpenTelemetry is central to the observability stack, collecting metrics, logs and traces from all system components. Prometheus, Grafana Loki and Tempo are used to monitor performance, store logs and manage traces, providing a comprehensive view of the system's health and performance.


2.4.1 Kubernetes
Kubernetes is the container orchestration platform at the heart of MATRA. It automates the deployment, scaling and management of containerized applications. 
Here are its key architectural components:
Control Plane: The "brain" of Kubernetes, responsible for managing the cluster's state and making decisions about workload placement and scheduling. Key components include:
    • API Server: The central point of interaction, handling all requests to the cluster.
    • Scheduler: Determines where to place new pods based on resource availability and constraints.
    • Controller Manager: Monitors the cluster's state and takes corrective actions to ensure it matches the desired state.
    • Etcd: A distributed key-value store that serves as the cluster's database, storing configuration data, state information and metadata about Kubernetes objects.
Nodes (Worker Machines): The physical or virtual machines where your applications run. Key components on each node include:
    • Kubelet: An agent that communicates with the Control Plane and manages the containers on the node.
    • Container Runtime: The software responsible for running containers (e.g., Docker, containerd).
    • Pods: The smallest deployable unit in Kubernetes, typically containing one or more containers that share resources (e.g., network, storage).


Services: An abstraction that provides a stable network endpoint for accessing a group of pods, enabling load balancing and service discovery.
Deployments: Manage the desired state of your application, ensuring the correct number of pods are running and handling updates and rollbacks.
StatefulSets: Manage the deployment and scaling of applications that require persistent storage and stable network identities, such as databases.
DaemonSets: Ensure that a copy of a pod runs on every node (or a selected subset) in the cluster, often used for system-level services like log collection or monitoring agents.
Other Objects: Kubernetes offers a rich ecosystem of objects for managing various aspects of your applications, including ConfigMaps, Secrets, Persistent Volumes and more.
2.4.2 Cilium (CNI)
Container Network Interface (CNI) is a framework used to dynamically configure network resources in Kubernetes. It utilizes a set of libraries, rules and specifications written in the Go programming language. This framework manages network traffic between nodes, pods and other Kubernetes resources.
CNI creates a virtual network layer on top of the physical network layer and manages network traffic between Kubernetes nodes through virtual network adapters. VXLAN (Virtual Extensible LAN) can be given as an example of this virtual layer. The diagram below illustrates the general working principles of CNIs.

In MATRA, the CNI will be used to establish the network infrastructure for Kubernetes clusters, which will form the foundational infrastructure for applications. Unlike traditional counterparts, the selected CNI will not utilize Kubernetes resources such as VXLAN, iptables and kube-proxy. Instead, it will manage the network directly at the kernel level using the eBPF (Extended Berkeley Packet Filtering) protocol, achieving faster performance with lower resource consumption.
2.4.3 Network Topology and BGP Configuration via Cilium (CNI) 
MATRA, which will be integrated into SCOMAR's existing network infrastructure, also leverages the network capabilities of external infrastructures. Although there is no physical load balance device in the network infrastructure, MATRA can provide load balancing functionality as if a physical load balance device were present. A BGP-supported Layer 3 device is sufficient for MATRA to operate effectively.
Providing load balancing features through Layer 3 devices reduces maintenance costs for MATRA users and offers a more flexible system infrastructure, especially when system reconfiguration or migration is necessary.
MATRA utilizes Border Gateway Protocol (BGP), which is commonly used in WAN network topologies to exchange routing tables with neighboring WANs. MATRA's BGP capabilities are provided by the Cilium CNI and it is used to advertise Kubernetes cluster resources across the network.
The Container Network Interface (CNI) has a crucial role in external load balancing by establishing BGP (Border Gateway Protocol) peering through a router or switch within its subnet. This allows the CNI to balance traffic directed to the Kubernetes cluster using the ECMP protocol at the L3 network layer (IP Layer), without the need for physical load balance devices.
Since load balancing requires more than one physical server, BGP-based load balancing cannot be implemented if there is only a single server on-site.


2.4.4 Traefik as Ingress Controller and API Gateway
Kubernetes Ingress is a resource that manages external access to services running in a Kubernetes cluster, typically HTTP and HTTPS routes. Ingress controllers, such as Traefik, implement the Ingress resource, providing a way to configure how traffic is routed to services. An Ingress controller watches the Kubernetes API for changes to Ingress resources and updates the routing rules accordingly. This allows for centralized management of routing within the cluster, making it easier to define and maintain how services are exposed to external users. Kubernetes Ingress supports features like SSL termination, path-based routing and load balancing, making it a powerful tool for managing traffic in a Kubernetes environment.


Traefik is an open-source edge router and ingress controller designed to manage incoming requests and route them to the appropriate services within a Kubernetes cluster. Its architecture is built around dynamic service discovery, automatically adjusting to changes in the environment such as new services being deployed or existing ones being updated. Traefik integrates seamlessly with various orchestration systems, particularly Kubernetes, where it can be deployed as a DaemonSet, ensuring that an instance runs on every node. This deployment model enhances resilience and scalability, allowing Traefik to handle traffic efficiently across the entire cluster. Traefik also provides built-in support for SSL/TLS termination, load balancing and middleware integration, which can modify requests and responses in transit.
Traefik leverages Cilium's external load balancer capability to manage traffic distribution across the cluster. This integration allows Traefik to obtain IP addresses from Cilium's IP pool, ensuring efficient and balanced routing of incoming requests to the appropriate services. By using Cilium's advanced networking features, Traefik can handle high volumes of traffic while maintaining low latency and high availability.
Traefik automatically discovers services within the Kubernetes cluster and routes traffic to the appropriate endpoints. This dynamic routing capability ensures that services remain accessible and can adapt to changes within the cluster without requiring manual updates.
Traefik's middleware feature allows for flexible request handling, enabling the integration of various functionalities such as authentication, rate limiting and custom headers. Middlewares are also used to enhance security and optimize the processing of incoming requests before they reach backend services.
Traefik manages SSL/TLS termination, ensuring that all communications between external clients and services are encrypted. This simplifies the management of secure connections and allows the system to handle certificates easily, including automatic renewals.


2.4.5 Software Defined Storages
Software Defined Storage (SDS) solutions are crucial for managing data storage in a flexible and scalable manner. SDS decouples storage resources from the underlying hardware, allowing for dynamic allocation based on application needs within the Kubernetes cluster.
Local Path Provisioner
The Local Path Provisioner is employed to manage persistent storage specifically for stateful sets, such as Kafka and databases like MongoDB and PostgreSQL. By using local disk space on each node, this provisioner ensures that these stateful applications have high-performance access to storage, essential for maintaining data integrity and performance. This approach is particularly beneficial for applications that need to consistently store and retrieve data locally on the nodes where they are running.



DirectPV for Direct Attached Storage
DirectPV is used to manage Direct Attached Storage (DAS), providing a direct mapping of storage devices to the Kubernetes cluster. This is ideal for high-performance applications that require low-latency access to storage without the overhead of networked storage solutions. DirectPV enables the efficient use of existing storage hardware, ensuring that applications can access the storage they need directly, which is particularly useful for workloads with high I/O demands. In MATRA, DirectPV is used for the Minio S3 cluster, enabling efficient access to storage for large volumes of object data. This ensures Minio S3 instances can operate with minimal latency and high performance.


Both Local Path Provisioner and DirectPV are fully integrated into the Kubernetes environment, allowing for seamless storage management alongside other cluster resources. This integration provides the flexibility needed to allocate storage dynamically, ensuring that applications have the necessary storage resources while optimizing overall storage efficiency.
The use of Software Defined Storage in MATRA ensures that storage resources are managed efficiently, providing the necessary performance and scalability to support critical applications like Kafka, databases and the Minio S3 cluster within the Kubernetes cluster.

2.4.6 Application and Service Level Security
Application security in MATRA is a critical component of the overall system architecture, ensuring that all elements within the Kubernetes environment are protected against unauthorized access, data breaches and other security threats. By implementing a multi-layered security strategy by using Vault , MATRA aims to safeguard both the infrastructure and the sensitive data it handles. This approach includes centralized management of secrets and certificates, strict access control policies, secure communications and continuous monitoring, all designed to maintain the integrity, confidentiality and availability of the system.



In MATRA, the management of public key infrastructure (PKI) and certificates is a critical aspect of the application’s security architecture. MATRA employs Vault and Cert-Manager to automate and secure the issuance, management and renewal of certificates, ensuring that all communications within the Kubernetes environment are encrypted and trusted.
Vault is configured to serve as a certificate authority (CA) through the creation of a Public Key Infrastructure (PKI) store. This setup allows MATRA to generate and manage certificates, which are crucial for securing communications between clients, users and systems. The PKI store in Vault can issue both self-signed certificates or certificates supplied by SCOMAR, offering flexibility in meeting different security requirements. Cert-Manager, which runs in every Kubernetes cluster, is integrated with Vault to automate the issuance and renewal of certificates, ensuring that all certificates are kept up-to-date without manual intervention. This automated approach reduces the risk of certificate expiration and helps maintain a secure environment across the system.


Vault integrates with Trust Manager to efficiently distribute CA certificates across the system, ensuring that all components operate under a unified certificate authority. This integration allows Vault to automatically propagate trusted CA certificates to every relevant service and environment, eliminating manual distribution and reducing the risk of configuration errors. By ensuring that all components consistently trust the same CA, the system maintains a cohesive security posture, preventing discrepancies that could lead to communication failures or security vulnerabilities. This seamless distribution mechanism not only simplifies certificate management but also enhances the overall trustworthiness and reliability of the system's security infrastructure.

Vault is also used to create key-value (KV) stores for managing secrets. These KV stores are designed to securely store sensitive information such as database passwords, API keys and other confidential data. By centralizing secret management in Vault, MATRA guarantees that sensitive data is protected with encryption at rest and in transit, reducing the risk of data breaches. Additionally, MATRA integrates Vault with External Secret Manager to create secrets directly within the Kubernetes clusters. This integration allows Kubernetes services to securely retrieve the secrets they need, ensuring that sensitive information is only accessible to authorized components.

The integration of Vault with MATRA’s overall security architecture ensures that both certificates and secrets are managed consistently and securely across all environments. By using Vault as the central authority for certificates and secrets, MATRA simplifies the management of sensitive data while enhancing the security of its infrastructure. The automated processes for certificate issuance and secret management reduce the administrative burden and help maintain a robust security framework. This comprehensive approach to certificate and secret management provides a strong foundation for protecting the integrity and confidentiality of the system’s most sensitive information.
2.4.7 Observability
In MATRA, observability is critical for ensuring the performance, reliability and overall health of the system, which includes shared services like Kafka, MongoDB and PostgreSQL, as well as microservices developed in Java, Node.js and other technologies. MATRA uses a comprehensive observability stack to collect, process and visualize metrics, logs and traces from across the entire infrastructure.
OpenTelemetry serves as the core of MATRA's observability architecture, enabling comprehensive data collection across the entire system. The architecture of OpenTelemetry in MATRA is designed to handle the collection of metrics, traces and logs from both microservices and shared services.
The system is built around the OpenTelemetry Collector, which acts as an intermediary between the various services and the storage and visualization platforms. The Collector receives data from the OpenTelemetry SDKs integrated within the microservices and shared services, processes this data and then exports it to the appropriate systems for storage and visualization. This architecture ensures that all observability data—whether metrics, traces, or logs—is consistently and efficiently collected, processed and made available for monitoring and analysis.



Metrics are essential for monitoring the performance and health of MATRA’s infrastructure. OpenTelemetry is at the core of the observability architecture, collecting metrics from all microservices and shared services. The OpenTelemetry Collector processes these metrics and exports them to Prometheus, which stores the data for monitoring and historical analysis. To ensure scalability and durability, Prometheus stores its collected metrics data in S3. This setup allows the MATRA to maintain a reliable and scalable metrics storage solution, enabling long-term storage and easy access for analysis over time.


Distributed tracing is a key aspect of observability in MATRA, providing detailed insights into how requests flow through the system. OpenTelemetry collects trace data, which is then managed by Grafana Tempo. Grafana Tempo stores distributed traces in S3, allowing the system administrator to analyze the interactions between services, particularly in the microservices architecture. This capability is crucial for diagnosing performance issues and understanding the dependencies between different services, enabling the system administrator to identify and resolve problems more effectively.



Logs capture detailed information about the system’s behavior and are important for troubleshooting and post-incident analysis. In MATRA, logs from all microservices and shared services are collected by OpenTelemetry collector and Grafana Loki stores them in S3. This approach ensures that logs are durable and can be accessed whenever needed. The integration of Grafana Loki with OpenTelemetry ensures that logs are consistently collected, making it easier to correlate them with metrics and traces. This comprehensive logging strategy allows the system administrator to investigate issues in-depth, helping to maintain the reliability and security of the system.



This observability stack, which includes OpenTelemetry, Prometheus, Grafana Loki, Tempo and Grafana, provides MATRA with the tools needed to effectively monitor, trace and log all aspects of the system. By collecting and analyzing metrics, traces and logs, MATRA ensures that any issues can be identified and resolved quickly, maintaining a high level of service reliability and availability.
2.4.8 Security Policies
Securing the Kubernetes Cluster, which serves as the core infrastructure for MATRA, requires a multi-layered and strategic approach. The architecture does not only support seamless operations but also ensure that security is embedded at every level, protecting both the infrastructure and the applications it hosts.
In the design of MATRA’s cluster architecture, Role-Based Access Control (RBAC) plays a critical role in securing access to the Kubernetes API. The RBAC architecture will be carefully designed to segment and restrict access based on roles, ensuring that only authorized and qualified personnel can perform administrative actions. Each role within the cluster will be defined according to the principle of least privilege, granting only the necessary permissions to the system administrator responsible for maintaining specific components of the system. This not only minimizes the risk of unauthorized access but also simplifies auditing and management of access controls across the environment.
The physical and operating system (OS) layers of the cluster designed to prevent unauthorized access and tampering. All physical ports on the Linux machines running the cluster, except those required by operation are disabled. This measure ensures that the hardware is protected from unauthorized connections that could compromise the cluster’s security.
At the OS level, the design incorporates strict control over superuser privileges. Only a system administrators are granted to superuser access and these privileges  monitored through detailed logging and auditing mechanisms. This ensures that any actions performed at the superuser level are fully traceable, preserving the integrity of the system.
To further enhance security at the cluster level, MATRA’s design includes kernel-level hardening of the Linux machines hosting the cluster. This involves integrating AppArmor security module within the kernel architecture, which enforces mandatory access controls at the lowest level. The kernel configuration set to disable unnecessary services and minimize the attack surface, thereby protecting the cluster from common vulnerabilities and potential exploits. By this configuration MATRA ensures that even if a service or process is compromised, its impact on the overall system is significantly limited.
MATRA supports regular and on-demand security audits, allowing for the identification and remediation of any vulnerabilities or misconfigurations. This approach ensures that the architecture remains secure and resilient over time.
The security architecture of the cluster aligned with industry best practices and compliance standards. This includes the implementation of security policies that are not only effective but also adaptable to evolving security requirements and threats. Regular reviews and updates to the security architecture will ensure that it remains robust and compliant, safeguarding both the infrastructure and the applications that depend on it.
2.5 Middlewares Architecture
In MATRA, middleware components play a vital role in enhancing the performance, scalability and reliability of the system. Middlewares like Redis, Kafka and Kafka Connect serve as the backbone for data caching, messaging and data integration processes. These components work together to ensure that data flows efficiently between microservices, external systems and databases, enabling real-time communication, reducing latency and ensuring data consistency across the entire system.
2.5.1 Redis
Redis is a middleware component in MATRA, primarily used for caching, geospatial queries and scaling real-time communications. By storing frequently accessed data in memory, Redis significantly reduces the load on databases and speeds up data retrieval, enhancing the overall performance of the system.
Requests frequently sent by users to the application interface are written to the database once and then stored in RAM on a Redis Sentinel cluster or designated storage. This approach aims to ensure rapid access and quick response to queries. As a result, the number of queries sent to the database is reduced and network traffic is less burdened.
Another reason for using Redis in MATRA is to facilitate faster geospatial queries, which enables quicker execution of tasks such as generating and classifying map data. Through Redis's geospatial indexing technique, any geospatial data query made by a client is intended to return results more effectively and rapidly, thereby saving time in the display of geographical data.
MATRA uses a hit-and-miss approach for caching. This method allows the system to check if a piece of data is already stored in the cache (a "hit") before querying the database. If the data is not found in the cache (a "miss"), it is retrieved from the database, stored in the cache and then returned. This approach significantly reduces the number of database queries, minimizing latency and improving overall system performance.

Additionally, Redis is used to scale web sockets through its pub/sub pattern. Web sockets are utilized as a communication protocol in scenarios requiring real-time data transmission, such as generating alarms or notifications. The scalability of web sockets is crucial for ensuring the fast and lossless transmission of real-time data. By using Redis's pub/sub pattern, a Redis instance functions as a central message broker. Messages published by any user are broadcast through the Redis pub/sub system via an open connection in web sockets. The pub/sub system ensures that these messages, published across different servers, are delivered to all connected subscribers. This approach guarantees that messages published on multiple servers are shared and communicated through the Redis pub/sub system, facilitating effective communication across the system.
MATRA uses a Redis Sentinel cluster to provide high availability and automated failover for Redis instances. Redis Sentinel monitors the health of the master and replica nodes, automatically promoting a replica to master if the current master node fails. This architecture ensures that Redis remains highly available and that data remains accessible even in the event of hardware or network failures. The Sentinel cluster continuously checks the status of each Redis instance, helping to maintain the stability and reliability of the system’s caching and real-time communication features.



2.5.2 Kafka
Apache Kafka is an open-source, high-performance distributed messaging system created by the Apache Foundation. It is designed for stream processing, real-time data pipelines, large-scale data integration, messaging and communication services. Kafka operates through servers and clients that communicate over the TCP network protocol and can be deployed on bare-metal environments, virtual machines and containers.



Kafka can operate using the publish/subscribe or producer/consumer model. To prevent data loss between brokers, Kafka uses the Raft consensus algorithm with at least three brokers, where members and a leader are designated. For each partition, one broker is selected as the leader. The leader broker receives the data for its partition and replicates it to the other two brokers for redundancy. If any broker fails, one of the other members is elected as the new leader, ensuring that data remains fault-tolerant, reliable and can be processed in real-time by the consumers.
Message producers in Kafka send messages to distributed brokers within the cluster, which are divided into partitions within topics. Producers publish these messages to specific topics and consumers subscribe to the desired topics to receive messages via the brokers. This architecture allows Kafka to efficiently manage and deliver messages across a distributed system, ensuring that data is processed and delivered reliably and in real-time, even in the event of broker failures.

Each partition in Kafka serves as a sequential, immutable log where messages are stored. Consumers read messages from these partitions and Kafka ensures that each message within a partition is processed by only one consumer in the group.
Records published by producers are written to partitions and stored chronologically across brokers. Each record is stored as a key-value pair, with optional timestamps and headers. Kafka groups these partitioned records under topics and delivers them to consumers based on the topics they have subscribed to. This design allows Kafka to maintain the order of records within partitions, ensuring that messages are processed efficiently and reliably.

KRaft is the consensus protocol that eliminates Kafka's dependency on ZooKeeper for data management. By integrating data responsibility directly within Kafka instead of splitting it between two different systems, KRaft simplifies Kafka's architecture. This unified approach streamlines data management and enhances the overall efficiency and reliability of the Kafka ecosystem.
Kafka stores data on disk, allowing it to retain messages for a configurable amount of time. This feature enables consumers to replay data, processing messages at their own pace and recovering from failures without losing data. In MATRA, this capability is critical for processing sensor data and ensuring that no data is lost during network outages or system failures.
In MATRA, Kafka acts as the central messaging hub, facilitating the flow of data between microservices. Its publish/subscribe model allows services to produce and consume messages independently, enabling loose coupling between components. This decoupling is critical in a microservices architecture, where services need to operate independently yet communicate seamlessly. By organizing data streams into topics, Kafka ensures that each service can subscribe to the specific data it needs, reducing unnecessary data processing and enhancing overall system efficiency. Its architectural role is multi-dimensional, offering a resilient foundation for both nearly real-time data processing and asynchronous communication.
Kafka is integrated with other systems in MATRA, such as Kafka Connect, which facilitates the import and export of data to and from Kafka topics. This integration allows Kafka to act as a central hub for data streams, seamlessly connecting microservices and external systems. Kafka’s flexibility and extensibility make it an essential component of MATRA’s data architecture, enabling efficient and reliable data flow across the entire system.
2.5.3 Kafka Connect
Kafka Connect is designed to facilitate the seamless integration of Kafka with various external systems. It serves as a framework for building and managing data pipelines between Kafka and other data systems, ensuring that data can be reliably and efficiently transferred in and out of Kafka. Kafka Connect supports scalable and fault-tolerant data movement, making it a critical tool for managing continuous data integration.
Kafka Connect enables the flow of data between Kafka and external systems, such as databases, file systems and messaging services. This process is typically accomplished using "source" connectors, which pull data from external systems into Kafka and "sink" connectors, which push data from Kafka to other systems. Kafka Connect comes with a rich ecosystem of reusable connector plugins, which simplify the process of integrating Kafka with a wide variety of data sources and destinations.

Kafka Connect acts as a bridge between Kafka and other systems, allowing for the continuous import and export of data. In MATRA, Kafka Connect is used to stream data into Kafka from external sources like SSLs and to export data from Kafka to target systems like CCC. This ensures that data is consistently available and synchronized across the entire architecture, supporting real-time data processing and integration.
Kafka Connect operates based on configuration files, typically defined in JSON or YAML format, which specify the data source, data format, connection details and other necessary parameters. These configuration files define whether the connector is a source (pushing data into Kafka) or a sink (pulling out from Kafka). Kafka Connect reads these configuration files and creates tasks according to the specified connector. Each task is responsible for processing a specific dataset and operates on threads managed by Kafka Connect. These threads handle the data flow, either pulling data from sources or pushing data to target systems.
By enabling real-time data processing, Kafka Connect ensures that data is always up-to-date and available for use by other services. This is particularly important in MATRA, where nearly real-time sensor data needs to be processed and made available to the CCC without delay. Kafka Connect facilitates this by continuously streaming data into Kafka from external sources and exporting it to other systems as needed.
Kafka Connect is designed to scale horizontally, allowing it to handle large volumes of data across multiple connectors. It supports both standalone and distributed modes, with the distributed mode providing the ability to scale out by adding more workers to the cluster. Kafka Connect also includes built-in fault tolerance, ensuring that data is processed reliably even in the event of failures.
Kafka Connect’s ability to manage data pipelines efficiently, combined with its scalability and reliability, makes it an essential tool in MATRA. It ensures that data flows seamlessly between Kafka and external systems, supporting the nearly real-time, continuous data integration required for MATRA’s success.

2.6 Databases and Object Storage Architecture
In MATRA, databases play a crucial role in managing both operational and structured data. MATRA uses combination of MongoDB for handling nearly real-time, dynamic data and PostgreSQL for managing data that requires high integrity and consistency. These databases ensure that data is stored, retrieved and processed efficiently, supporting the various services and applications within the system. By leveraging the strengths of both MongoDB and PostgreSQL, MATRA achieves a robust and flexible data management architecture that meets the diverse needs of the system.
2.6.1 MongoDB
MongoDB is a critical component in MATRA, designed to manage real-time, dynamic operational data with a flexible and scalable architecture. Its architecture is designed to handle large volumes of data with high performance and flexibility, making it well-suited for MATRA’s needs, which include real-time data processing from various sources such as sensors, user interactions and system events.
MongoDB is a NoSQL database that stores data in flexible, JSON-like documents. This document-based storage model allows for the easy handling of complex and varying data structures, which is essential for the diverse and dynamic nature of the data managed in MATRA. Each document in MongoDB can contain nested structures, arrays and different data types, making it possible to store and query complex data efficiently.
Collections are the primary storage units in MongoDB, where documents are stored. Unlike tables in relational databases, collections do not enforce a schema, allowing for greater flexibility in data modeling. Each document in a collection can have a different structure, which is ideal for the diverse and evolving data needs of MATRA. For example, different types of data can be stored in the same collection, even if they have different fields. Collections also support powerful indexing options, making it easier to query and analyze data efficiently.



MongoDB supports a wide range of indexing options, including single field, compound, geospatial and text indexes, which are used to optimize query performance. İndexing is used extensively to ensure that queries on operational data, such as track information or user activity logs, are processed quickly. The ability to create custom indexes allows MongoDB to meet the specific performance requirements of different queries, ensuring that data retrieval is both fast and efficient. The flexible document model of MongoDB makes it easier to handle the varied data formats produced by these sources, enabling nearly real-time analysis and decision-making.


MongoDB supports multi-document transactions to meet more complex use cases. A transaction in MongoDB allows multiple operations on one or more documents to be executed as a single, atomic operation. This is particularly useful in MATRA when ensuring data consistency across multiple collections or when performing complex operations that must be completed together or not at all. For example, in scenarios where updates to multiple related documents are necessary, MongoDB’s transaction support ensures that either all updates are applied, or none are, maintaining data integrity.
To ensure high availability, in MATRA, MongoDB uses replica sets, which are groups of MongoDB servers that maintain the same data. In a replica set, one node is designated as the primary node, which receives all write operations, while the other nodes are secondary replicas that replicate the data from the primary node. If the primary node fails, one of the secondary nodes is automatically promoted to primary, ensuring that the database remains available without interruption. This architecture is very important for MATRA, where uninterrupted access to operational data is necessary for real-time decision-making.



In MATRA, microservices access MongoDB via a Kubernetes service and when a primary change occurs, the MongoDB Operator automatically updates the service to reflect the new primary, ensuring continuous access to the database.


To ensure data durability and disaster recovery, MongoDB data is regularly backed up to S3. This integration with S3 provides a secure and reliable way to store backups, ensuring that critical data is protected and can be restored quickly in the event of data loss or corruption. The use of S3 for backups aligns with MATRA's emphasis on maintaining high availability and data integrity.



MongoDB's architecture in MATRA is tailored to handle the demands of nearly real-time data processing, providing a flexible, scalable and reliable platform for managing dynamic operational data. Its use of replica sets ensures high availability and fault tolerance, while regular backups to S3 enhance data durability and disaster recovery capabilities.
2.6.2 PostgreSQL
PostgreSQL is an open-source relational database that supports SQL standards and is fully ACID-compliant (Atomicity, Consistency, Isolation, Durability). Its open-source nature makes PostgreSQL cost-effective in terms of maintenance, management and licensing, while also providing easy access to a wide range of operational and usage resources.
In MATRA, PostgreSQL is used to store data that requires high integrity and consistency but does not involve constant read and write operations. This includes critical information such as user authentication data and access tokens. PostgreSQL ensures that this data remains secure, consistent and readily accessible whenever need.
PostgreSQL enforces data integrity through the use of constraints, such as primary keys, foreign keys, unique constraints and check constraints. These features ensure that the data adheres to the defined schema rules, preventing invalid or inconsistent data from being stored. 
PostgreSQL in MATRA is managed using a PostgreSQL Kubernetes Operator, which automates many of the database's operational tasks, including replication and high availability. The PostgreSQL Operator configures the database in a three-replica setup to ensure that the system remains available even in the event of a failure. This replication setup ensures that the database remains available even if one or more instances fail, providing redundancy and minimizing downtime. In the event of a failure, one of the replicas can take over, ensuring continuous service and data availability.
Microservices access the PostgreSQL master via a Kubernetes service. If the primary database node fails, the PostgreSQL Operator automatically promotes a secondary node to primary and updates the Kubernetes service to point to the new master, ensuring continuous data availability without manual intervention.


Regular backups of PostgreSQL data are critical for disaster recovery. In MATRA, backups are taken periodically and stored securely in S3 Object Storage. This setup ensures that all critical data is protected and can be restored quickly in the event of data loss or corruption. PostgreSQL's support for point-in-time recovery (PITR) allows the database to be restored to a specific moment, providing flexibility and reliability in recovery scenarios.



PostgreSQL’s architecture in MATRA is designed to provide a stable, secure and efficient environment for managing structured data. Its support for relational integrity, advanced querying, replication and backup processes ensures that the database meets the stringent requirements for data consistency and availability.

2.6.3 Minio (S3 Object Storage)
Minio serves as the S3-compatible object storage solution in MATRA, providing a scalable, high-performance platform for storing large volumes of data, including video recordings from EO cameras. The architectural design of Minio within MATRA ensures that data is stored securely and is readily accessible for both live streaming and playback.
Minio is deployed as a distributed system within the Kubernetes cluster, allowing it to scale horizontally by adding more instances as needed. This distribution ensures that data is replicated across multiple nodes, providing high availability and fault tolerance. By deploying Minio in this manner, the system can handle large datasets efficiently, ensuring that data is always available even in the event of node failures.

Minio stores data in the form of objects within buckets. Each object is identified by a unique key within its bucket, making it easy to store, retrieve and manage data. The system is designed to handle large files, such as video recordings and provides fast access for replay scenarios. Minio’s S3-compatible API allows for seamless integration with existing tools and workflows, ensuring that data can be accessed using standard S3 commands and libraries.
Minio is designed to scale horizontally, meaning that as storage demands grow, additional Minio instances can be added to the cluster without disruption. This scalability ensures that the system can handle increasing amounts of data over time, maintaining performance levels even as the dataset expands. Minio’s architecture also includes built-in load balancing, which helps distribute requests evenly across all available storage nodes, optimizing resource utilization.
Minio’s distributed architecture includes erasure coding, a feature that enhances data redundancy by breaking data into fragments, expanding it with redundant pieces and distributing it across multiple drives and nodes. This ensures that even if some nodes or drives fail, data can still be reconstructed and recovered without loss.

Minio supports bucket-to-bucket replication, a critical feature for MATRA, especially for ensuring data availability across multiple sites. In this setup, data stored in one Minio bucket (e.g., at a SSL) is automatically replicated to another bucket (e.g., at the CCC). This replication process ensures that video recordings from EO cameras and other critical objects are synchronized between locations, providing a backup that is immediately accessible in case of network outages or local failures. This feature is essential for maintaining data consistency and availability across the distributed infrastructure of MATRA.

2.7 Identity Management
Identity management is a critical component that ensures secure and efficient access to various services and resources within the system. MATRA leverages advanced identity management technologies to handle user authentication, authorization and access control, ensuring that only authorized users can access sensitive data and perform specific actions. The integration of robust identity management solutions helps maintain the integrity and security of the overall system, while also simplifying the management of users, roles and permissions across the platform.
2.7.1 ORY Hydra
ORY Hydra is an open-source OAuth 2.0 and OpenID Connect server used in MATRA to manage user authentication and secure access to various services. It plays a vital role in the identity management architecture by providing a reliable and scalable solution for handling authentication and authorization flows.
ORY Hydra serves as the core authentication server in MATRA, implementing the OAuth 2.0 and OpenID Connect standards. It is responsible for issuing access tokens, refresh tokens and ID tokens, which are used by services and clients to authenticate users and authorize their actions. When a user attempts to access a service, ORY Hydra verifies their identity and provides the necessary tokens to allow or deny access based on predefined policies and permissions.
It's primary function is to manage tokens, which are central to the OAuth 2.0 and OpenID Connect protocols. Tokens issued by ORY Hydra are used by services to validate user sessions and authorize access to resources. ORY Hydra supports the storage and retrieval of tokens in PostgreSQL, ensuring that tokens are securely managed and easily accessible when needed. This token management process includes features like token expiration, revocation and refresh, all of which are crucial for maintaining the security of the system.



ORY Hydra integrates seamlessly with MATRA's user management system, KAPI, which is responsible for managing users, roles and permissions. ORY Hydra uses this data to authenticate users and issue tokens. This integration ensures that user authentication is consistent and secure across the entire system. 
ORY Hydra is highly extensible, allowing MATRA to customize authentication flows according to specific needs. This includes integrating with external identity providers, customizing login and consent pages and implementing additional security checks. ORY Hydra's flexibility ensures that it can adapt to the unique requirements of MATRA, providing a tailored authentication and authorization solution.

2.7.2 OPA
OPA (Open Policy Agent) plays an important role in centralized authorization within MATRA by providing a unified, consistent approach to enforcing access control policies across all services and applications. By centralizing authorization decisions, OPA ensures that all requests to access resources are evaluated against the same set of policies, maintaining security and compliance throughout the system.
OPA centralizes the management of access control policies by allowing all authorization rules to be defined and managed in one place. These policies are written in Rego, OPA’s policy language and can cover a wide range of conditions and scenarios. By centralizing these policies, OPA eliminates the need for individual services to implement and manage their own access control logic, reducing the risk of inconsistencies and simplifying policy updates.
With OPA, all authorization decisions are made consistently, regardless of which service or application is being accessed. When a user or service attempts to access a resource, the request is intercepted by OPA, which evaluates it against the centralized policies. This ensures that the same rules are applied across the entire system, providing a uniform security posture.
One of the key benefits of OPA’s centralized approach is the ability to update policies dynamically without redeploying services. In MATRA, KAPI user management module, which handles roles and permissions, can update OPA’s Rego policies whenever there are changes. This means that as the operation’s security needs evolve or as users’ roles change, the authorization policies can be updated in real-time, ensuring that access control remains aligned with the latest requirements.

OPA works closely with the identity management system, particularly ORY Hydra, which handles authentication. After a user is authenticated, OPA steps in to make the authorization decision, determining whether the authenticated user has the necessary permissions to access the requested resource. This integration ensures that both authentication and authorization are handled in a cohesive, centralized manner, further enhancing the security of MATRA.
OPA’s architecture in MATRA provides a robust, scalable and flexible solution for managing authorization across the entire system. Its integration with Kubernetes and other services ensures that access control policies are consistently enforced, enhancing the security, compliance and operational efficiency 
2.7.3 User Authentication and Authorization Flow
MATRA manages user authentication and authorization through a series of components working harmoniously. The system comprises Traefik, Ory Hydra, Policy Agent (OPA), KAPI and HAP, which operate sequentially to handle the entire authentication and authorization process.
Users are authenticated before gaining access to applications. The applications themselves function independently of the authentication process but are integrated into the overall system. The authentication and authorization workflow follows the steps outlined below, ensuring that access is managed securely and efficiently.
Authentication Steps:
    1. The user sends a request to the gateway (Traefik).
    2. Traefik redirects the request to the OpenID/OAuth2 provider (Ory Hydra).
    3. The OpenID/OAuth2 provider redirects the traffic to the login API defined in KAPI using an HTTP 302 status code. 
    4. KAPI replies with a status code to Traefik.
    5. Traefik forwards the response to the web browser.
    • Initial Access Token Check Completed
    6. The login screen appears in the browser and the user provides their credentials.
    7. The login request reaches the gateway, which then redirects it to KAPI to validate the credentials.
    8. KAPI sends a login authentication request to the OpenID/OAuth2 provider and receives a redirect URI.
    9. The traffic is redirected to the gateway with the redirect URI and an authorization code (HTTP 302).
    10.  The gateway responds to the web browser with an HTTP 302 status, including the URI and authorization code.
    11.  The browser requests an access token using the authorization code through the gateway.
    12. The traffic is redirected to the OpenID/OAuth2 provider.
    13.  The provider responds with an access token to the browser via the gateway and the token is stored in the browser's local storage.
    • Authentication Completed
Access and Authorization:
    14. The browser sends a request to the HAP backend services with the access token through the gateway.
    15. The gateway redirects the traffic to the Open Policy Agent (OPA) middleware, which validates the access token.
    16. Once the token is verified, the traffic is redirected to OPA to assign permissions to the access token.
    17. The policies running on OPA evaluate the client request to determine whether the client has permission to access the respective services.
In Case of Unauthorized Request:
    18. a. The middleware responds with an HTTP 401 status code to the gateway.
    19. a. The gateway forwards the response to the browser.
In Case of Authorized Request:
    18. b. The request, now containing only the username in the header, is redirected to HAP.
    19. b. HAP responds to the gateway.
    20. The gateway forwards the response to the browser.
This formalized description outlines the sequential process of user authentication and authorization, highlighting how each component interacts within the system.
2.8 Media Server
In MATRA, the media server is responsible for streaming, recording and playing back video from EO cameras, making it an integral part of the system's video management. The server is specifically utilized to stream live video feeds from EO cameras to users, record these streams to S3 storage and provide the capability to replay the recorded streams from S3 as needed.
The media server handles the continuous streaming of EO camera feeds, ensuring low-latency delivery to users, which is crucial for real-time monitoring and decision-making. It supports various streaming protocols, including WebRTC, RTMP and HLS to accommodate different network conditions and device capabilities, ensuring that the video streams reach users efficiently and with minimal delay.
The media server is deployed using a multi-level cluster configuration, which is designed to enhance scalability, redundancy and performance. In this setup, media servers are strategically placed on SSLs, at the CCC and within the DMZ. This deployment ensures that video streams are delivered through the shortest path possible, reducing latency and improving the user experience. For instance, users at SSL can stream video directly from the local media server, while users at the CCC can access video streams from the theirs.


By deploying the media server within a Kubernetes cluster, the system benefits from Kubernetes' ability to automatically manage and scale the media server instances based on demand. This ensures that video streams are delivered efficiently, with the nearest media server handling the stream to minimize latency. 
In addition to streaming, the media server records the video streams directly to S3 object storage. This integration with S3 allows for scalable and durable storage of video data, ensuring that recorded streams are securely stored and can be accessed for future use. The media server also facilitates the playback of recorded streams directly from S3. This capability is vital for reviewing past events with providing access to historical video data.

By incorporating the media server with these capabilities, MATRA ensures robust and reliable video management, providing users with real-time access to EO camera feeds and the ability to review recorded streams as needed.

2.9 Data Flow
The data flow in MATRA is designed to ensure efficient and secure movement of data between various components within the system. This section outlines how data is transmitted, processed and stored across the different layers of the architecture.
2.9.1 Client – Server Communication
Users interact with the system through web interfaces, sending HTTPS requests that are first routed through Traefik, which acts as the API gateway. Traefik handles the initial request processing, including SSL/TLS termination and directs the requests to the appropriate backend services based on predefined routing rules. For real-time communication needs, WebSockets are used to maintain a persistent connection between the client and the server. 

Before a request reaches its target microservice, it undergoes authentication and authorization checks. Authentication is handled by ORY Hydra, working with KAPI, which verifies the user's identity using OAuth2 and OpenID Connect protocols. Once authenticated, the request is forwarded to OPA (Open Policy Agent) for authorization, where it is checked against the policies that define the user’s permissions. Only if the user is authorized to access the requested resource does the request proceed to the microservice. (This will be detailed in 2.11 Security and Networking)
After the backend service processes the request, the response follows a similar path back through the system. The microservice generates a response, which is sent back through Traefik. Traefik may apply additional routing or transformations if necessary and the response is then sent to the client. For real-time data, WebSockets are used to push updates directly to the client over the persistent connection. Finally, the authenticated response is delivered to the user, ensuring that all security and authorization checks remain intact throughout the process.
2.9.2 Inter-Service Communication
In MATRA, communication between microservices is carefully designed to ensure efficiency, reliability and scalability within the distributed architecture. The system utilizes both synchronous and asynchronous communication methods, tailored to the specific requirements of each interaction, ensuring that services can operate independently while maintaining consistent and timely data exchanges.
For synchronous request-response interactions, MATRA employs RESTful APIs. This method is used when immediate feedback is required, such as in user authentication, data retrieval, or when services need to query each other for up-to-date information. RESTful communication is built on HTTP, a widely adopted protocol that facilitates straightforward and reliable interactions between services. By using REST, the system leverages a standard approach that is well-understood and supported across various platforms and languages, making it easier to integrate and extend as new services are developed.



To handle scenarios where services can operate independently of each other’s timing, MATRA uses Apache Kafka for asynchronous communication. Kafka enables services to publish and subscribe to streams of records, allowing them to process data asynchronously. This method is particularly effective for handling event processing, data pipelines, or tasks that do not require an immediate response.
 Kafka’s role in the architecture is to decouple services, allowing them to perform their functions without waiting for others to complete theirs. This decoupling enhances the scalability of the system, as services can be scaled independently based on their specific workloads. For example, when sensor data is ingested and needs to be processed by different services (e.g., data analysis, storage, or display), Kafka ensures that each service can consume the data at its own pace, processing it as resources become available.
In scenarios where tasks need to be performed asynchronously, such as batch processing, long-running operations, or event-driven workflows, Kafka’s message queues are used to manage these workloads. Services publish messages to Kafka topics, which are then processed by one or more consumer services. This approach not only balances the load across services but also provides fault tolerance, as messages can be retained in Kafka until they are successfully processed, even if a service is temporarily unavailable.


While Kafka is primarily used for asynchronous communication, MATRA also leverages Kafka’s reply-to pattern for scenarios that require near-synchronous communication. This pattern allows services to send a request through Kafka and receive a response via a separate Kafka topic, enabling asynchronous processing with a synchronous-like experience. This is useful in cases where a service needs to request data from another service and proceed only after receiving the necessary information, but where the interaction does not need to occur in real-time.
Kafka’s built-in fault tolerance and high availability are critical to ensuring reliable communication between microservices. With replication and partitioning, Kafka ensures that even if some brokers fail, the system remains operational and no data is lost. This reliability is crucial in MATRA, where consistent and accurate data flow between services is necessary to maintain system integrity and performance.
The microservices communication strategy in MATRA is designed to support the scalability, flexibility and reliability of the overall system. By combining RESTful APIs for synchronous communication with Kafka for asynchronous messaging, the architecture allows services to interact efficiently while maintaining independence, ensuring that the system can scale and adapt to meet evolving demands.
2.9.3 Data Replication Between Sites
Data replication between sites is designed to ensure data availability, consistency and fault tolerance across geographically distributed locations. MATRA involves CCC and several SSL sites, all of which generate and process significant amounts of data, including sensor data from radars, meteo sensors and EO cameras. To maintain seamless operations and ensure that data is always accessible, even in the event of network disruptions, the architecture employs robust replication strategies for both sensor data and video recordings.
Sensor Data Replication with Kafka Connect
Sensor data, which is processed and then written to Kafka at each site, requires reliable replication to ensure that the CCC has access to all relevant data. In MATRA, Kafka Connect is used to facilitate this replication. Kafka Connect at CCC continuously fetches raw and processed sensor data from the local Kafka clusters at each SSL. This data is then replicated to CCC’s local Kafka cluster, ensuring that the CCC has a complete and up-to-date view of the sensor information.
This replication process is designed to handle large volumes of data efficiently and to ensure that no data is lost, even in the event of network outages. Kafka’s distributed architecture and fault tolerance features ensure that data replication continues smoothly, with messages being retained until they can be successfully transmitted and processed.

Video Recording Replication with S3
The EO cameras at each site generate substantial amounts of video data, which is recorded by a media server and stored in S3 buckets. To ensure that this critical video data is available at the CCC, MATRA employs a bucket-to-bucket replication strategy within S3. Each SSL’s S3 bucket is configured to replicate its contents to the corresponding S3 bucket at CCC. This replication is automatic and continuous, ensuring that all video recordings are duplicated at the CCC.
This strategy is particularly important for maintaining access to video data in the event of network outages. Should a network disruption occur, the local S3 bucket at a SSL continues to store video data and once connectivity is restored, the replication process resumes, transferring any accumulated data to the CCC. This ensures that no video data is lost and that the CCC can always access the latest recordings.

Network Outage Resilience
The data replication strategies employed in MATRA are specifically designed to handle network outages between sites. By using Kafka for sensor data and S3’s bucket-to-bucket replication for video recordings, the system ensures that data is not lost or delayed during network disruptions. When the network is restored, both Kafka Connect and S3 replication automatically synchronize the data, ensuring that the CCC has a complete and accurate dataset.These replication strategies ensure that users at CCC can access all relevant data, including real-time sensor data and EO camera recordings, even during or after network issues. By maintaining local copies of the data at sites, the system ensures that operations can continue uninterrupted, with all necessary information available for decision-making and analysis.
The data replication architecture in MATRA is a robust solution for maintaining data consistency and availability across geographically distributed sites. By leveraging Kafka Connect for sensor data and S3’s replication capabilities for video data, the system ensures that critical information is always accessible, resilient to network disruptions and capable of supporting the operational needs of the CCC and SSLs.
2.10 External Systems Integration
Integrations described in technical specifications will be based on OCTOPUS integration infrastructure located in DMZ. OCTOPUS is an integration infrastructure developed upon open architecture principles which enables addition or customization of new or existing integration points. 
OCTOPUS enables integration through configurable routes based on EIPs (Enterprise Integration Patterns). OCTOPUS uses Apache Camel for EIP implementations. Octopus routes consists of three modules gateways, transforms and data.
Data: Data modules contains classes representing integration data. The data received through integration points will be stored in those data beans.
Gateway: Gateway modules are responsible for establishing initial connection in order to send/receive data, serialize/deserialize data to and from data objects.
Transform: Transform modules are responsible to convert gateway data objects to intended object type and send them to Apache Kafka for display. The transformations will be based on rule-based configurations that can be configured according to new requirements.


Figure 1. Integration Data Flow
Figure above represents the data flow through octopus modules (gateway, transform etc…). The whole integration chain for all integration points is depicted in Figure 1.
According to Figure 1, gateway modules establish connections with integration points. Data coming from integration points is processed in gateway modules, deserialized to data objects and enqueued to queue in order to be processed by transform modules. Transform modules will process the awaiting data bean from the queue, transform to SCOMAR data structure and enqueue to another queue. Then the processed SCOMAR data will be dequeued from the queue and published via Apache Kafka in order to be displayed. 
Figure 2 depicts the general integration points and data flow through Octopus. According to the figure, SCOMAR will receive AIS information from Romanian Naval Authority (ANR). AIS data will be received through TCP/IP via AIS gateway and transformed to track data and routed to Kafka in order to be displayed in SCOMAR. Similarly, radar and ais data coming from Ministry of National Defence will be received in the same way described for ANR.
Alert information will be received from eNave module via web service gateway and transformed into SCOMAR notification data and routed to Kafka in order to be processed in SCOMAR system.
In addition to the retrieved data SCOMAR will provide information in two ways according to the nature of information. One of the integration methods will be based on REST services via provided API allowing consumers to request data with/without filter. Other integration method will be based on TCP in order to provide streaming data such as AIS and Radar. All integration services will be protected via authentication mechanism in order to prevent data output without permission.

2.11 Redundancy Strategies
In MATRA, redundancy strategies are critical for ensuring system availability, data integrity and operational continuity, especially in the face of hardware failures, data loss, or other unexpected events. By implementing a combination of hardware redundancy, data replication and robust backup strategies, the system is designed to minimize risks and maintain uninterrupted service, even under challenging conditions.
2.11.1 Hardware Redundancy Strategies
Disk and Storage Redundancy 
To prevent data loss from potential disk failures, MATRA employs a combination of RAID and Erasure Coding technologies. RAID provides hardware-level redundancy by distributing data across multiple disks, allowing the system to continue functioning even if a disk fails. Erasure Coding further enhances data resilience by breaking data into fragments, adding redundancy and distributing it across different disks or nodes, enabling data reconstruction even if multiple disks fail. 
Additionally, critical data is periodically backed up to external storage, providing an extra layer of protection. This approach ensures that data remains safe and recoverable in the event of significant hardware failures or system outages.
Power Redundancy
All servers are equipped with redundant power supply units (PSUs), each connected to separate Power Distribution Units (PDUs). This dual-connection setup ensures that if one PSU or PDU experiences a failure, the other can seamlessly maintain power, preventing any interruption to server operations.
In the SCOMAR Data Center and the Main Data Center, one of the PDUs is connected to a newly installed Uninterruptible Power Supply (UPS) system. This UPS provides critical backup power in the event of a primary power source failure, allowing the servers to continue operating without interruption. Meanwhile, at SSLs (Shelter Locations), the PDUs are connected to the existing redundant UPS systems within the shelter, which have been designed to provide continuous power during outages.
By integrating redundant PSUs, PDUs and UPS systems across all key locations, the power redundancy strategy ensures that the MATRA’s infrastructure remains resilient to power fluctuations and outages, maintaining operational continuity even in adverse conditions.
Network Redundancy
To ensure uninterrupted network connectivity and mitigate the risks associated with network card or cable failures, MATRA implements a comprehensive network redundancy strategy. Each server is outfitted with two independent network cards (NICs), each featuring at least two ports. This dual-NIC configuration provides multiple paths for network traffic, ensuring that if one network card or its associated cables fail, the other card can maintain the connection without disruption.
The ports on these network cards are redundantly connected to separate, redundant network switches. This setup not only protects against NIC or cable failures but also ensures that the network remains operational even if a switch experiences a failure. By creating multiple layers of redundancy—at both the network card and switch levels—this strategy provides a robust safeguard against potential network issues, ensuring continuous connectivity and minimizing the risk of network-related outages in MATRA’s infrastructure.
2.11.2 Data Replication Strategies
Within the scope of the SCOMAR project, sensor data in SSL will be replicated to CCC to prevent data loss. Details regarding replication are explained under the heading Data Replication Between SSLs and CCC.
2.11.3 Data Backup Strategies
The backup solution for the SCOMAR Project has been designed to prevent data loss, ensure data integrity and provide rapid data recovery. In the infrastructure that will operate on bare metal Kubernetes, the aim is to provide a high backup rate using an open-source solution. The current environment is located on a Kubernetes cluster, with data stored on server disks. The backup storage unit is a separate physical backup storage accessible over the network via the NFS protocol.
A container-based backup solution will be used on Kubernetes to back up the applications. This solution has the capability to automatically back up resources and persistent volumes within the Kubernetes cluster. For databases, periodic backups will be performed using not only the container-based backup solution but also their own backup tools, leveraging the automation and scheduling features of these tools.
The installation of the backup solution will begin with the deployment of backup agents on the nodes within the Kubernetes cluster. These agents will periodically transfer data from the cluster to the designated backup server. Both the backup server and the database servers will access the backup storage via the NFS protocol.
Daily and weekly backup intervals will be established. Applications on Kubernetes are planned to be backed up daily and databases on a similar schedule. Weekly full backups will be performed, while daily backups will capture only the changed data. All backup operations will be securely and systematically stored on the backup storage accessed via NFS. In the event that data restoration is required, the backup solution will facilitate quick data recovery processes.
Data security is a crucial aspect of our backup solution. Backup data will be encrypted and stored on the backup storage. Specialized monitoring tools will be employed to ensure seamless backup operations and periodic restore tests will be conducted to verify the accuracy and usability of the backups.
First and second-level support groups will be assigned to address potential issues that may arise during or after the backup processes. Technical documentation and operational procedures related to the backup software and NFS access will be prepared. It is crucial that this plan is regularly reviewed and implemented to maintain data integrity and business continuity.
In conclusion, this backup solution design is structured to reliably back up the existing infrastructure and ensure rapid data recovery when necessary. The backup strategies and methods align with MATRA's objectives and needs and will be periodically reviewed and updated.
